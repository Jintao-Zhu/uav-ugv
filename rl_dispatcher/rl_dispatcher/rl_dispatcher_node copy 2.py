import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
# 1.21 ä¸‹é¢æ³¨é‡Šçš„æ˜¯æ˜¨å¤©zjtæ”¹çš„ï¼Œä¸Šé¢æ˜¯æˆ‘æŠŠçŠ¶æ€æ”¹æˆ45ç»´çš„ï¼Œè·‘å‡ºçš„ç»“æœè¿˜æ˜¯ä¸€æ ·ï¼Œåˆ†äº†å…­ä¸ªä»»åŠ¡ç»™å°è½¦1
import rclpy
from rclpy.node import Node
from rmf_fleet_msgs.msg import FleetState, RobotMode  # æ–°å¢ï¼šå¯¼å…¥RobotModeæšä¸¾
from std_msgs.msg import String
from geometry_msgs.msg import Point
from rmf_custom_tasks_self.srv import SingleNavTask
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, StopTrainingOnRewardThreshold
from stable_baselines3.common.evaluation import evaluate_policy
import time
import random
import threading
import torch as th

# ========== å½»åº•æ¶ˆé™¤Gymè­¦å‘Š ==========
import warnings
warnings.filterwarnings("ignore")
import gymnasium as gym
from gymnasium import spaces

# å¼•å…¥æ¨¡æ‹Ÿç¯å¢ƒ
try:
    from mock_env import MockRMFEnv
except ImportError:
    print("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° mock_env.pyï¼Œè¯·ç¡®ä¿æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼")
    sys.exit(1)

# ===================== çœŸå® ROS ç¯å¢ƒ =====================
class RLDispatchingEnv(gym.Env):
    """
    çœŸå®RMFç¯å¢ƒï¼šåŒ¹é…Mocké€»è¾‘ï¼Œ8ä»»åŠ¡å¿…é¡»å®Œæˆ+æ— å¾…å‘½ç‚¹è¿”å›
    """
    metadata = {"render_modes": ["human"], "render_fps": 1}
    
    def __init__(self, node, render_mode=None):
        super().__init__()
        self.node = node
        self.render_mode = render_mode
        
        # 1. å°è½¦é…ç½®ï¼ˆæ— å¾…å‘½ç‚¹è¿”å›ï¼‰
        self.robot_names = ["deliveryRobot_0", "deliveryRobot_1", "deliveryRobot_2"]
        self.robot_positions = {
            "deliveryRobot_0": Point(x=96.59527587890625, y=-51.96450424194336),
            "deliveryRobot_1": Point(x=152.3477325439453, y=-44.31863021850586),
            "deliveryRobot_2": Point(x=14.776845932006836, y=-9.279278755187988)
        }
        self.robot_idle = {name: True for name in self.robot_names}
        self.robot_task_map = {}
        self.robot_remaining_time = {name: 0.0 for name in self.robot_names}
        
        # 2. ä»»åŠ¡é˜Ÿåˆ—ï¼ˆä¿ç•™æ‰€æœ‰ä»»åŠ¡ï¼Œä¸ä¸¢å¼ƒï¼‰
        self.pending_tasks = []
        self.executing_tasks = {}
        self.completed_tasks = []
        self.task_timeout_count = {}  # è®°å½•ä»»åŠ¡è¶…æ—¶æ¬¡æ•°ï¼Œé¿å…é‡å¤æƒ©ç½š
        
        # 3. ç»Ÿè®¡å˜é‡
        self.current_time = 0.0
        self.episode_steps = 0
        self.max_episode_steps = 1000
        self.total_reward = 0.0
        
        # 4. çŠ¶æ€ç©ºé—´ï¼ˆæ¢å¤45ç»´ï¼ŒåŒ¹é…è®­ç»ƒå¥½çš„æ¨¡å‹ï¼‰
        self.action_space = spaces.Discrete(len(self.robot_names) + 1)
        self.observation_space = spaces.Box(low=-1.0, high=1.0, shape=(45,), dtype=np.float32)
        
        # 5. å¥–åŠ±ç³»æ•°ï¼ˆä¸Mockå®Œå…¨å¯¹é½ï¼šä¸‹è°ƒå°ºåº¦åï¼‰
        self.reward_coeff = {
            "distance": -0.02,
            "completion": 30.0,
            "batch_completion": 20.0,
            "idle_selection": -1.0,
            "timeout": -5.0,
            "step": -0.001,
            "wait_short": 0.2,
            "wait_long": -0.1,
            "all_completed": 100.0,
            "valid_selection": 1.0
        }

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.robot_idle = {name: True for name in self.robot_names}
        self.robot_task_map = {}
        self.robot_remaining_time = {name: 0.0 for name in self.robot_names}
        self.pending_tasks = []
        self.executing_tasks = {}
        self.completed_tasks = []
        self.task_timeout_count = {}
        self.current_time = 0.0
        self.episode_steps = 0
        self.total_reward = 0.0
        return self._get_observation(), {"total_reward": 0.0}

    def _get_observation(self):
        """æ¢å¤45ç»´çŠ¶æ€å‘é‡ï¼ˆåŒ¹é…è®­ç»ƒæ¨¡å‹çš„è¾“å…¥ç»´åº¦ï¼‰"""
        # 1. æœºå™¨äººçŠ¶æ€ï¼ˆ12ç»´ï¼‰
        robot_obs = []
        for name in self.robot_names:
            robot_obs.extend([
                self.robot_positions[name].x / 200.0,
                self.robot_positions[name].y / 100.0,
                1.0 if self.robot_idle[name] else 0.0,
                min(self.robot_remaining_time[name] / 200.0, 1.0)
            ])
        
        # 2. ä»»åŠ¡çŠ¶æ€ï¼ˆ24ç»´ï¼š8ä¸ªä»»åŠ¡ï¼‰
        task_obs = []
        for _ in range(8):
            task_obs.extend([0.0, 0.0, 0.0])
        
        for i, task in enumerate(self.pending_tasks[:8]):
            task_obs[i*3] = task["x"] / 200.0
            task_obs[i*3+1] = task["y"] / 100.0
            task_obs[i*3+2] = min(task["wait_time"] / 200.0, 1.0)
        
        # 3. å…¨å±€çŠ¶æ€ï¼ˆ9ç»´ï¼šæ¢å¤åˆ é™¤çš„2ä¸ªç‰¹å¾ï¼Œæ€»ç»´åº¦å›åˆ°45ï¼‰
        idle_robot_count = sum([1 for name in self.robot_names if self.robot_idle[name]])
        robot_remaining_times = [min(self.robot_remaining_time[name]/200.0, 1.0) for name in self.robot_names]
        global_obs = [
            self.current_time / 1000.0,          # 1
            len(self.completed_tasks) / 8.0,     # 2
            len(self.pending_tasks) / 8.0,       # 3
            idle_robot_count / 3.0,              # 4
            len(self.executing_tasks) / 8.0,     # 5ï¼ˆæ¢å¤åˆ é™¤çš„ç‰¹å¾ï¼‰
            sum(robot_remaining_times) / 3.0,    # 6ï¼ˆæ¢å¤åˆ é™¤çš„ç‰¹å¾ï¼‰
            *robot_remaining_times               # 7-9ï¼ˆ3ä¸ªæœºå™¨äººå‰©ä½™æ—¶é—´ï¼‰
        ]
        
        # æ€»ç»´åº¦ï¼š12 + 24 + 9 = 45ï¼ˆåŒ¹é…è®­ç»ƒæ¨¡å‹ï¼‰
        return np.array(robot_obs + task_obs + global_obs, dtype=np.float32)

    def step(self, action):
        self.episode_steps += 1
        self.current_time += 1.0
        reward = self.reward_coeff["step"]
        
        # å¥–åŠ±è£å‰ªå‡½æ•°
        def clip_reward(r, min_r=-10.0, max_r=50.0):
            return max(min(r, max_r), min_r)
        
        wait_action_idx = len(self.robot_names)
        info = {
            "action_type": "wait" if action == wait_action_idx else "assign",
            "selected_action": action,
            "idle_robots": sum([1 for name in self.robot_names if self.robot_idle[name]])
        }
        
        # --- å¤„ç†ç­‰å¾…åŠ¨ä½œ ---
        if action == wait_action_idx:
            if self.pending_tasks:
                # æ›´æ–°ä»»åŠ¡ç­‰å¾…æ—¶é—´
                for task in self.pending_tasks:
                    task["wait_time"] += 1.0
                
                # ç­‰å¾…å¥–åŠ±/æƒ©ç½š
                first_task_wait = self.pending_tasks[0]["wait_time"]
                if first_task_wait < 10.0:
                    reward += self.reward_coeff["wait_short"]
                    reward = clip_reward(reward)
                elif first_task_wait > 30.0:
                    reward += self.reward_coeff["wait_long"]
                    reward = clip_reward(reward)
                
                # è¶…æ—¶å¤„ç†ï¼šä»…æƒ©ç½šï¼Œä¸ä¸¢å¼ƒä»»åŠ¡
                idle_robot_count = sum([1 for name in self.robot_names if self.robot_idle[name]])
                if idle_robot_count > 0:
                    for task in self.pending_tasks:
                        tid = task["task_id"]
                        if task["wait_time"] > 120.0 and self.task_timeout_count.get(tid, 0) == 0:
                            reward += self.reward_coeff["timeout"]
                            reward = clip_reward(reward)
                            self.task_timeout_count[tid] = 1
        
        # --- å¤„ç†é€‰è½¦åŠ¨ä½œ ---
        else:
            selected_robot = self.robot_names[action]
            info["selected_robot"] = selected_robot
            
            if not self.pending_tasks:
                truncated = self.episode_steps >= self.max_episode_steps
                return self._get_observation(), reward, False, truncated, info
            
            # é€‰è·ç¦»æœ€è¿‘çš„ä»»åŠ¡
            if self.robot_idle[selected_robot]:
                # é€‰ç©ºé—²è½¦çš„æ­£å‘å¥–åŠ±
                reward += self.reward_coeff["valid_selection"]
                reward = clip_reward(reward)
                
                min_dist = float("inf")
                best_task_idx = 0
                for i, task in enumerate(self.pending_tasks):
                    rx = self.robot_positions[selected_robot].x
                    ry = self.robot_positions[selected_robot].y
                    dist = np.sqrt((rx - task["x"])**2 + (ry - task["y"])**2)
                    if dist < min_dist:
                        min_dist = dist
                        best_task_idx = i
                
                task = self.pending_tasks.pop(best_task_idx)
                task_id = task["task_id"]
                info["task_id"] = task_id
                
                # æä½çš„è·ç¦»æƒ©ç½š
                reward += self.reward_coeff["distance"] * min_dist
                reward = clip_reward(reward)
                
                # è®¡ç®—å‰©ä½™æ‰§è¡Œæ—¶é—´
                self.robot_remaining_time[selected_robot] = dist / 1.0
                
                # æ›´æ–°çŠ¶æ€
                self.robot_idle[selected_robot] = False
                self.robot_task_map[selected_robot] = task_id
                self.executing_tasks[task_id] = {
                    "robot": selected_robot,
                    "start_time": self.current_time,
                    "waypoint": task["waypoint"],
                    "distance": min_dist
                }
            else:
                # é€‰å¿™ç¢Œè½¦ï¼šæè½»æƒ©ç½š
                reward += self.reward_coeff["idle_selection"]
                reward = clip_reward(reward)
        
        # --- å®Œæˆä»»åŠ¡å¥–åŠ± ---
        self.total_reward += reward
        truncated = self.episode_steps >= self.max_episode_steps
        
        return self._get_observation(), reward, False, truncated, info

    # è¾…åŠ©æ–¹æ³•ï¼šROS æ•°æ®æ›´æ–°
    def update_robot_position(self, name, x, y):
        if name in self.robot_positions:
            self.robot_positions[name].x = x
            self.robot_positions[name].y = y
            
    def add_task(self, tid, wp, x, y):
        # æ–°å¢ä»»åŠ¡æ—¶åˆå§‹åŒ–è¶…æ—¶è®¡æ•°
        self.pending_tasks.append({
            "task_id": tid, 
            "waypoint": wp, 
            "x": x, 
            "y": y, 
            "wait_time": 0.0
        })
        self.task_timeout_count[tid] = 0
        
    def complete_task(self, tid):
        if tid in self.executing_tasks:
            robot = self.executing_tasks[tid]["robot"]
            # ä»»åŠ¡å®Œæˆæ ¸å¿ƒå¥–åŠ±
            self.total_reward += self.reward_coeff["completion"]
            self.completed_tasks.append(tid)
            # é‡ç½®å°è½¦çŠ¶æ€ï¼ˆåœåœ¨ä»»åŠ¡ç‚¹ï¼Œä¸è¿”å›å¾…å‘½ç‚¹ï¼‰
            self.robot_idle[robot] = True
            self.robot_remaining_time[robot] = 0.0
            del self.executing_tasks[tid]
            self.node.get_logger().info(f"ğŸ’° ä»»åŠ¡å®Œæˆå¥–åŠ± +{self.reward_coeff['completion']}! (Robot: {robot})")
            
            # å®Œæˆæ‰€æœ‰8ä¸ªä»»åŠ¡ï¼šè¶…å¤§å¥–åŠ±
            if len(self.completed_tasks) == 8:
                self.total_reward += self.reward_coeff["all_completed"]
                self.node.get_logger().info(f"ğŸ‰ å®Œæˆæ‰€æœ‰8ä¸ªä»»åŠ¡ï¼é¢å¤–å¥–åŠ± +{self.reward_coeff['all_completed']}")

# ===================== æ ¸å¿ƒè°ƒåº¦èŠ‚ç‚¹ =====================
class RLDispatcherNode(Node):
    def __init__(self):
        super().__init__("rl_dispatcher_node")
        self.get_logger().info("ğŸš€ åˆå§‹åŒ– RL è°ƒåº¦èŠ‚ç‚¹ï¼ˆ8ä»»åŠ¡å¿…é¡»å®Œæˆç‰ˆï¼‰...")
        
        # 1. ä»¿çœŸæ—¶é—´é…ç½®
        if not self.has_parameter("use_sim_time"):
            self.declare_parameter("use_sim_time", True)
        self.set_parameters([rclpy.parameter.Parameter("use_sim_time", rclpy.Parameter.Type.BOOL, True)])

        # 2. æ¨¡å¼é…ç½®
        if not self.has_parameter("mode"):
            self.declare_parameter("mode", "train")
        self.mode = self.get_parameter("mode").get_parameter_value().string_value
        self.get_logger().info(f"ğŸ”µ å½“å‰æ¨¡å¼: [{self.mode.upper()}]")
        
        # 3. åŒå¼•æ“åˆ‡æ¢
        if self.mode == "train":
            self.get_logger().info("ğŸš€ è®­ç»ƒæ¨¡å¼ - æŒ‚è½½ Mock è™šæ‹Ÿå¼•æ“")
            self.rl_env = MockRMFEnv() 
        elif self.mode == "infer":
            self.get_logger().info("ğŸ¤– æ¨ç†æ¨¡å¼ - æŒ‚è½½ Real çœŸå®å¼•æ“")
            self.rl_env = RLDispatchingEnv(self, render_mode="human")
        else:
            self.get_logger().error("âŒ æœªçŸ¥æ¨¡å¼ï¼è¯·ä½¿ç”¨ mode:=train æˆ– mode:=infer")
            sys.exit(1)

        # 4. åˆå§‹åŒ– RL æ¨¡å‹ï¼ˆè°ƒä¼˜è¶…å‚æ•°ï¼Œè§£å†³è®­ç»ƒéœ‡è¡ï¼‰
        self.model = None
        self._init_rl_model()
        
        # 5. æ¨ç†æ¨¡å¼åˆå§‹åŒ– ROS æ¥å£
        if self.mode == "infer":
            self._init_ros_interfaces()

    def _init_ros_interfaces(self):
        """åˆå§‹åŒ– ROS è®¢é˜…/å‘å¸ƒ/æœåŠ¡"""
        self.get_logger().info("ğŸ”Œ è¿æ¥ ROS æ¥å£...")
        
        # èˆªç‚¹åæ ‡æ˜ å°„
        self.waypoint_coords = {
            "n14": Point(x=80.84, y=-28.52), "n13": Point(x=84.44, y=-4.94),
            "n23": Point(x=182.80, y=-42.30), "s08": Point(x=96.61, y=-50.50),
            "s10": Point(x=122.10, y=-46.68), "west_koi_pond": Point(x=34.32, y=-10.13),
            "n08": Point(x=59.61, y=-7.42), "junction_south_west": Point(x=84.56, y=-38.81)
        }

        # è®¢é˜…å™¨ï¼ˆä»…ä¿ç•™ä½ å·²éªŒè¯å¯ç”¨çš„ï¼‰
        self.create_subscription(FleetState, "/fleet_states", self.fleet_state_callback, 10)
        self.create_subscription(String, "/task_monitor/start", self.target_callback, 10)
        self.create_subscription(String, "/custom_task_completion", self.completion_callback, 10)
        
        # æœåŠ¡å®¢æˆ·ç«¯
        self.nav_client = self.create_client(SingleNavTask, "/submit_single_nav_task")
        
        # å†³ç­–å®šæ—¶å™¨ï¼ˆ1ç§’/æ¬¡ï¼‰
        self.create_timer(1.0, self.infer_dispatch_callback)
        
        # å»é‡ç¼“å­˜
        self.processed_ids = set()
        
        # æ–°å¢ï¼šè®°å½•æ­£åœ¨è·¯ä¸Šçš„è¯·æ±‚ï¼Œé˜²æ­¢é‡å¤æäº¤
        self.dispatching_task_ids = set() 
        
        self.get_logger().info("âœ… ROS æ¥å£è¿æ¥å®Œæˆ")

    def _init_rl_model(self):
        """åˆå§‹åŒ– PPO æ¨¡å‹ï¼ˆè°ƒä¼˜è¶…å‚æ•°ï¼Œè§£å†³KLæ•£åº¦è¿‡ä½ã€å¥–åŠ±éœ‡è¡é—®é¢˜ï¼‰"""
        model_path = "./rl_models/ppo_800000_steps"
        log_dir = "./rl_dispatching_tb_log/"
        
        if self.mode == "train":
            # è½»é‡åŒ–ç½‘ç»œï¼ˆé¿å…è¿‡æ‹Ÿåˆï¼‰
            policy_kwargs = dict(
                activation_fn=th.nn.ReLU,
                net_arch=dict(pi=[128, 128], vf=[128, 128])
            )
            self.get_logger().info("ğŸ“š åˆå§‹åŒ– PPO æ¨¡å‹ï¼ˆè°ƒä¼˜è¶…å‚æ•°ç‰ˆï¼‰...")
            self.model = PPO(
                "MlpPolicy", 
                self.rl_env, 
                verbose=1, 
                device='cpu', 
                tensorboard_log=log_dir,
                policy_kwargs=policy_kwargs,
                learning_rate=1e-4,        # ä»3e-5â†’1e-4ï¼šæé«˜å­¦ä¹ ç‡ï¼Œå¢å¼ºç­–ç•¥æ›´æ–°
                n_steps=2048,              # ä¿æŒä¸å˜
                batch_size=64,             # ä¿æŒä¸å˜
                gamma=0.99,                # ä¿æŒä¸å˜
                gae_lambda=0.95,           # ä¿æŒä¸å˜
                ent_coef=0.05,             # ä»0.01â†’0.05ï¼šæé«˜ç†µç³»æ•°ï¼Œå¢å¼ºæ¢ç´¢
                vf_coef=0.5,               # ä¿æŒä¸å˜
                max_grad_norm=0.5,         # ä¿æŒä¸å˜
                n_epochs=10,               # ä¿æŒä¸å˜
                clip_range=0.3             # ä»0.2â†’0.3ï¼šæ‰©å¤§è£å‰ªèŒƒå›´ï¼Œè§£å†³KLæ•£åº¦è¿‡ä½
            )
            # å›è°ƒå‡½æ•°ï¼šç›‘æ§æ”¶æ•›ï¼Œä¿å­˜æœ€ä¼˜æ¨¡å‹
            self.checkpoint_callback = CheckpointCallback(save_freq=20000, save_path="./rl_models/", name_prefix="ppo")
            
            # å¥–åŠ±é˜ˆå€¼åœæ­¢å›è°ƒï¼ˆä½œä¸ºEvalCallbackçš„å­å›è°ƒï¼‰
            self.stop_callback = StopTrainingOnRewardThreshold(
                reward_threshold=20000.0,  # è®¾å®šåˆç†å¥–åŠ±é˜ˆå€¼
                verbose=1
            )
            
            # EvalCallbackï¼šå°†åœæ­¢å›è°ƒä½œä¸ºå­å›è°ƒä¼ å…¥
            self.eval_callback = EvalCallback(
                self.rl_env,
                eval_freq=5000,
                n_eval_episodes=5,
                best_model_save_path="./rl_models/best/",
                verbose=1,
                callback_after_eval=self.stop_callback
            )
            
        elif self.mode == "infer":
            if os.path.exists(model_path + ".zip"):
                self.get_logger().info(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {model_path}")
                self.model = PPO.load(model_path, device='cpu')
            else:
                self.get_logger().error(f"âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}.zip")
                self.model = None

    # è®­ç»ƒé€»è¾‘
    def start_training(self, total_timesteps=1000000):
        if self.mode != "train": return
            
        self.get_logger().info(f"ğŸ”¥ å¼€å§‹è®­ç»ƒ {total_timesteps} æ­¥ï¼ˆ8ä»»åŠ¡å¿…é¡»å®Œæˆç‰ˆï¼‰...")
        start_t = time.time()
        
        self.model.learn(
            total_timesteps=total_timesteps, 
            callback=[self.checkpoint_callback, self.eval_callback]
        )
        
        # è¯„ä¼°æœ€ç»ˆæ¨¡å‹
        mean_reward, std_reward = evaluate_policy(self.model, self.rl_env, n_eval_episodes=10)
        self.get_logger().info(f"ğŸ“Š æœ€ç»ˆæ¨¡å‹è¯„ä¼°ï¼šå¹³å‡å¥–åŠ±={mean_reward:.2f}ï¼Œæ ‡å‡†å·®={std_reward:.2f}")
        
        self.model.save("./rl_models/dispatching_ppo_final")
        self.get_logger().info("ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ï¼")
        rclpy.shutdown()

    # æ¨ç†é€»è¾‘ï¼ˆæ ¸å¿ƒï¼šè¿‡æ»¤å·²æ‰§è¡Œ/æ­£åœ¨æäº¤çš„ä»»åŠ¡ï¼Œé˜²æ­¢é‡å¤ï¼‰
    def infer_dispatch_callback(self):
        # è¿‡æ»¤æ¡ä»¶ï¼šä»…ä¿ç•™æœªæ‰§è¡Œã€æœªæäº¤çš„red_cubeä»»åŠ¡
        valid_tasks = [
            t for t in self.rl_env.pending_tasks 
            if t["task_id"].startswith("red_cube_") 
            and t["task_id"] not in self.rl_env.executing_tasks
            and t["task_id"] not in self.dispatching_task_ids
        ]
        if not valid_tasks:
            self.get_logger().debug("ğŸ“­ æ— æœ‰æ•ˆä»»åŠ¡ï¼Œè·³è¿‡è°ƒåº¦")
            return
        
        # æ¨¡å‹å†³ç­–
        obs = self.rl_env._get_observation()
        action = 0
        if self.model:
            action, _ = self.model.predict(obs, deterministic=True)
        
        wait_action_idx = len(self.rl_env.robot_names)
        
        # å¤„ç†ç­‰å¾…åŠ¨ä½œ
        if action == wait_action_idx:
            self.get_logger().info("â³ æ¨¡å‹å†³ç­–ï¼šç­‰å¾…æ–°ä»»åŠ¡ï¼ˆå‡‘å•åæ‰¹é‡è°ƒåº¦ï¼‰")
            for task in self.rl_env.pending_tasks:
                task["wait_time"] += 1.0
            return
        
        # å¤„ç†é€‰è½¦åŠ¨ä½œ
        selected_robot = self.rl_env.robot_names[action]
        
        # å®‰å…¨é™çº§ï¼šé€‰å¿™ç¢Œè½¦åˆ™åˆ‡æ¢åˆ°ç©ºé—²è½¦
        if not self.rl_env.robot_idle[selected_robot]:
            idle_indices = [i for i, name in enumerate(self.rl_env.robot_names) if self.rl_env.robot_idle[name]]
            if idle_indices:
                action = random.choice(idle_indices)
                selected_robot = self.rl_env.robot_names[action]
                self.get_logger().warn(f"ğŸ›¡ï¸ ä¿®æ­£ï¼šé€‰ç©ºé—²è½¦ {selected_robot}")
        
        # é€‰è·ç¦»æœ€è¿‘çš„ä»»åŠ¡
        min_dist = float("inf")
        best_task = None
        for task in valid_tasks:
            rx = self.rl_env.robot_positions[selected_robot].x
            ry = self.rl_env.robot_positions[selected_robot].y
            dist = np.sqrt((rx - task["x"])**2 + (ry - task["y"])**2)
            if dist < min_dist:
                min_dist = dist
                best_task = task
        
        if best_task:
            tid = best_task["task_id"]
            
            # é˜²æŠ–ï¼šæ­£åœ¨æäº¤çš„ä»»åŠ¡è·³è¿‡
            if tid in self.dispatching_task_ids:
                self.get_logger().debug(f"â³ ä»»åŠ¡ {tid} æ­£åœ¨è¯·æ±‚ä¸­ï¼Œè·³è¿‡...")
                return

            # æ ‡è®°ä¸ºæ­£åœ¨æäº¤
            self.dispatching_task_ids.add(tid)
            
            # è°ƒç”¨æœåŠ¡ä¸‹å‘ä»»åŠ¡
            self._call_ros_service(selected_robot, tid, best_task["waypoint"])

    def _call_ros_service(self, robot, tid, wp):
        if not self.nav_client.service_is_ready():
            self.get_logger().warn("âš ï¸ æœåŠ¡æœªå°±ç»ªï¼Œè·³è¿‡ä»»åŠ¡ä¸‹å‘")
            # ç§»é™¤æ ‡è®°ï¼Œé¿å…æ°¸ä¹…é”å®š
            self.dispatching_task_ids.discard(tid)
            return
            
        req = SingleNavTask.Request()
        req.target_waypoint = wp
        req.fleet_name = "deliveryRobot"
        req.robot_name = robot
        req.priority = 1
        
        future = self.nav_client.call_async(req)
        future.add_done_callback(lambda f: self._service_done(f, tid, robot))

    def _service_done(self, future, tid, robot):
        """ä¿®å¤ï¼šæ— è®ºæˆåŠŸå¤±è´¥ï¼Œåªè¦æœ‰ç»“æœäº†ï¼Œå°±ä»â€œå‘é€ä¸­â€åå•ç§»é™¤ï¼Œå¹¶é˜²æ­¢å­—ç¬¦ä¸²åˆ‡å‰²æŠ¥é”™"""
        self.dispatching_task_ids.discard(tid)

        try:
            res = future.result()
            if res.success:
                self.get_logger().info(f"ğŸš€ è°ƒåº¦æˆåŠŸï¼š{robot} -> {tid} @ {res.message}")
                
                # éå† pending_tasks æ‰¾åˆ°å¯¹åº”çš„ä»»åŠ¡å¯¹è±¡
                target_task_idx = -1
                for i, t in enumerate(self.rl_env.pending_tasks):
                    if t["task_id"] == tid:
                        target_task_idx = i
                        break
                
                if target_task_idx != -1:
                    # å–å‡ºä»»åŠ¡æ•°æ®
                    task_data = self.rl_env.pending_tasks.pop(target_task_idx)
                    
                    # æ›´æ–°çŠ¶æ€
                    self.rl_env.robot_idle[robot] = False
                    self.rl_env.robot_task_map[robot] = tid
                    
                    # æ ¸å¿ƒä¿®å¤ï¼šç›´æ¥ä½¿ç”¨ task_data ä¸­çš„ waypointï¼Œä¸è¦ split ID
                    wp_name = task_data["waypoint"] 
                    
                    # è®¡ç®—è·ç¦»
                    if wp_name in self.waypoint_coords:
                        target_point = self.waypoint_coords[wp_name]
                        current_pos = self.rl_env.robot_positions[robot]
                        dist = np.sqrt((current_pos.x - target_point.x)**2 + (current_pos.y - target_point.y)**2)
                    else:
                        dist = 0.0 # å…œåº•

                    self.rl_env.executing_tasks[tid] = {
                        "robot": robot,
                        "start_time": self.rl_env.current_time,
                        "waypoint": self.waypoint_coords.get(wp_name, Point()), # é˜²æ­¢ç©ºæŒ‡é’ˆ
                        "distance": dist
                    }
            else:
                self.get_logger().error(f"âŒ è°ƒåº¦å¤±è´¥ï¼š{tid} -> {res.message}")
        except Exception as e:
            self.get_logger().error(f"âŒ æœåŠ¡è°ƒç”¨å¼‚å¸¸ï¼š{e}")
            import traceback
            traceback.print_exc()

    def fleet_state_callback(self, msg):
        """æ ¸å¿ƒä¿®å¤ï¼šåŸºäºä½ å®é™…çš„ RobotState å­—æ®µåˆ¤æ–­ç©ºé—²çŠ¶æ€"""
        if msg.name != "deliveryRobot": return
        for r in msg.robots:
            if r.name in self.rl_env.robot_names:
                # 1. æ›´æ–°å°è½¦ä½ç½®ï¼ˆä¿ç•™ï¼‰
                self.rl_env.update_robot_position(r.name, r.location.x, r.location.y)
                
                # 2. ä¿®å¤ï¼šåŸºäºå®é™…å­—æ®µåˆ¤æ–­ç©ºé—²çŠ¶æ€ï¼ˆæ— task_stateï¼Œç”¨mode+task_idï¼‰
                # RMF RobotModeæšä¸¾å€¼ï¼š1=IDLE, 2=MOVING, 3=PAUSED, 4=CHARGING
                is_idle = (r.mode.mode == RobotMode.MODE_IDLE) and (not r.task_id) and (r.battery_percent > 0.0)
                self.rl_env.robot_idle[r.name] = is_idle
                
                # 3. åŒæ­¥å‰©ä½™æ‰§è¡Œæ—¶é—´ï¼ˆä¿ç•™ï¼‰
                if not is_idle and r.task_id:
                    if r.name in self.rl_env.robot_task_map:
                        tid = self.rl_env.robot_task_map[r.name]
                        if tid in self.rl_env.executing_tasks:
                            wp = self.rl_env.executing_tasks[tid]["waypoint"]
                            current_pos = Point(x=r.location.x, y=r.location.y)
                            dist = np.sqrt((current_pos.x - wp.x)**2 + (current_pos.y - wp.y)**2)
                            self.rl_env.robot_remaining_time[r.name] = dist / 1.0
                else:
                    self.rl_env.robot_remaining_time[r.name] = 0.0

    def target_callback(self, msg):
        """å¤„ç†ä»»åŠ¡å‘å¸ƒè¯é¢˜ï¼Œæ–°å¢ä»»åŠ¡åˆ°å¾…æ‰§è¡Œé˜Ÿåˆ—"""
        data = msg.data.split(",")
        if len(data) < 2:
            self.get_logger().warn(f"âš ï¸ æ— æ•ˆä»»åŠ¡æ ¼å¼ï¼š{msg.data}")
            return
        
        tid, wp = data[0].strip(), data[1].strip()
        # å»é‡ï¼šå·²å¤„ç†çš„ä»»åŠ¡ä¸å†æ·»åŠ 
        if not tid.startswith("red_cube_") or tid in self.processed_ids:
            return
        
        if wp in self.waypoint_coords:
            p = self.waypoint_coords[wp]
            self.rl_env.add_task(tid, wp, p.x, p.y)
            self.processed_ids.add(tid)
            self.get_logger().info(f"ğŸ“¥ æ–°å¢ä»»åŠ¡ï¼š{tid} @ {wp}")
        else:
            self.get_logger().error(f"âŒ æœªçŸ¥èˆªç‚¹ï¼š{wp}ï¼Œä»»åŠ¡{tid}æ·»åŠ å¤±è´¥")

    def completion_callback(self, msg):
        """å¤„ç†ä»»åŠ¡å®Œæˆè¯é¢˜ï¼Œé‡ç½®å°è½¦çŠ¶æ€"""
        data = msg.data.split(",")
        if len(data) < 1:
            self.get_logger().warn(f"âš ï¸ æ— æ•ˆçš„å®Œæˆæ¶ˆæ¯ï¼š{msg.data}")
            return
        tid = data[0].strip()
        
        if tid.startswith("red_cube_"):
            # è°ƒç”¨ç¯å¢ƒçš„å®Œæˆä»»åŠ¡æ–¹æ³•
            self.rl_env.complete_task(tid)
            # æ¸…ç†ç¼“å­˜
            if tid in self.processed_ids:
                self.processed_ids.remove(tid)
            # å¼ºåˆ¶é‡ç½®å¯¹åº”å°è½¦çŠ¶æ€ï¼ˆå…œåº•ï¼‰
            for robot in self.rl_env.robot_names:
                if self.rl_env.robot_task_map.get(robot, "") == tid:
                    self.rl_env.robot_idle[robot] = True
                    self.rl_env.robot_remaining_time[robot] = 0.0
                    del self.rl_env.robot_task_map[robot]
                    break

# ä¸»ç¨‹åº
def main(args=None):
    rclpy.init(args=args)
    node = RLDispatcherNode()
    
    if node.mode == "train":
        train_thread = threading.Thread(
            target=node.start_training, 
            args=(1000000,),
            daemon=True
        )
        train_thread.start()
        try:
            rclpy.spin(node)
        except SystemExit:
            pass
        train_thread.join()
    else:
        try:
            rclpy.spin(node)
        except KeyboardInterrupt:
            pass
        
    node.destroy_node()
    if rclpy.ok():
        rclpy.shutdown()

if __name__ == "__main__":
    main()

# #!/usr/bin/env python3
# #1.20ï¼Œzjtæ”¹
# # é€‚é…éœ€æ±‚ï¼š8ä»»åŠ¡å¿…é¡»å®Œæˆ+æ— å¾…å‘½ç‚¹è¿”å›+è¶…æ—¶ä¸ä¸¢ä»»åŠ¡
# # æ ¸å¿ƒä¼˜åŒ–ï¼šä¸‹è°ƒå¥–åŠ±ç³»æ•°ã€è°ƒä¼˜PPOè¶…å‚æ•°ï¼Œè§£å†³è®­ç»ƒéœ‡è¡é—®é¢˜
# # ä¿®æ”¹rlèŠ‚ç‚¹ï¼Œç¡®ä¿å…¶å’Œrmfæ­£å¸¸äº¤äº’ï¼Œä¿è¯gazeboé‡Œå°è½¦çš„é€»è¾‘æ˜¯æŒ‰ç…§rlçš„å†³ç­–ç»“æœæ¥çš„
# import sys
# import os
# sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# import rclpy
# from rclpy.node import Node
# from rmf_fleet_msgs.msg import FleetState, RobotMode  # æ–°å¢ï¼šå¯¼å…¥RobotModeæšä¸¾
# from std_msgs.msg import String
# from geometry_msgs.msg import Point
# from rmf_custom_tasks_self.srv import SingleNavTask
# import numpy as np
# from stable_baselines3 import PPO
# from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, StopTrainingOnRewardThreshold
# from stable_baselines3.common.evaluation import evaluate_policy
# import time
# import random
# import threading
# import torch as th

# # ========== å½»åº•æ¶ˆé™¤Gymè­¦å‘Š ==========
# import warnings
# warnings.filterwarnings("ignore")
# import gymnasium as gym
# from gymnasium import spaces

# # å¼•å…¥æ¨¡æ‹Ÿç¯å¢ƒ
# try:
#     from mock_env import MockRMFEnv
# except ImportError:
#     print("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° mock_env.pyï¼Œè¯·ç¡®ä¿æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼")
#     sys.exit(1)

# # ===================== çœŸå® ROS ç¯å¢ƒ =====================
# class RLDispatchingEnv(gym.Env):
#     """
#     çœŸå®RMFç¯å¢ƒï¼šåŒ¹é…Mocké€»è¾‘ï¼Œ8ä»»åŠ¡å¿…é¡»å®Œæˆ+æ— å¾…å‘½ç‚¹è¿”å›
#     """
#     metadata = {"render_modes": ["human"], "render_fps": 1}
    
#     def __init__(self, node, render_mode=None):
#         super().__init__()
#         self.node = node
#         self.render_mode = render_mode
        
#         # 1. å°è½¦é…ç½®ï¼ˆæ— å¾…å‘½ç‚¹è¿”å›ï¼‰
#         self.robot_names = ["deliveryRobot_0", "deliveryRobot_1", "deliveryRobot_2"]
#         self.robot_positions = {
#             "deliveryRobot_0": Point(x=96.59527587890625, y=-51.96450424194336),
#             "deliveryRobot_1": Point(x=152.3477325439453, y=-44.31863021850586),
#             "deliveryRobot_2": Point(x=14.776845932006836, y=-9.279278755187988)
#         }
#         self.robot_idle = {name: True for name in self.robot_names}
#         self.robot_task_map = {}
#         self.robot_remaining_time = {name: 0.0 for name in self.robot_names}
        
#         # 2. ä»»åŠ¡é˜Ÿåˆ—ï¼ˆä¿ç•™æ‰€æœ‰ä»»åŠ¡ï¼Œä¸ä¸¢å¼ƒï¼‰
#         self.pending_tasks = []
#         self.executing_tasks = {}
#         self.completed_tasks = []
#         self.task_timeout_count = {}  # è®°å½•ä»»åŠ¡è¶…æ—¶æ¬¡æ•°ï¼Œé¿å…é‡å¤æƒ©ç½š
        
#         # 3. ç»Ÿè®¡å˜é‡
#         self.current_time = 0.0
#         self.episode_steps = 0
#         self.max_episode_steps = 1000
#         self.total_reward = 0.0
        
#         # 4. çŠ¶æ€ç©ºé—´ï¼ˆä¸Mockä¸€è‡´ï¼š45ç»´ -> ä¿®å¤ä¸º43ç»´é€‚é…æ—§æ¨¡å‹ï¼‰
#         self.action_space = spaces.Discrete(len(self.robot_names) + 1)
#         # ã€ä¿®æ”¹ç‚¹1ã€‘å°† shape=(45,) æ”¹å› shape=(43,)
#         self.observation_space = spaces.Box(low=-1.0, high=1.0, shape=(43,), dtype=np.float32)
        
#         # 5. å¥–åŠ±ç³»æ•°ï¼ˆä¸Mockå®Œå…¨å¯¹é½ï¼šä¸‹è°ƒå°ºåº¦åï¼‰
#         self.reward_coeff = {
#             "distance": -0.02,
#             "completion": 30.0,
#             "batch_completion": 20.0,
#             "idle_selection": -1.0,
#             "timeout": -5.0,
#             "step": -0.001,
#             "wait_short": 0.2,
#             "wait_long": -0.1,
#             "all_completed": 100.0,
#             "valid_selection": 1.0
#         }

#     def reset(self, seed=None, options=None):
#         super().reset(seed=seed)
#         self.robot_idle = {name: True for name in self.robot_names}
#         self.robot_task_map = {}
#         self.robot_remaining_time = {name: 0.0 for name in self.robot_names}
#         self.pending_tasks = []
#         self.executing_tasks = {}
#         self.completed_tasks = []
#         self.task_timeout_count = {}
#         self.current_time = 0.0
#         self.episode_steps = 0
#         self.total_reward = 0.0
#         return self._get_observation(), {"total_reward": 0.0}

#     def _get_observation(self):
#         """ã€ä¿®æ”¹ç‚¹2ã€‘å›é€€åˆ°43ç»´çŠ¶æ€å‘é‡ï¼Œé€‚é…æ—§æ¨¡å‹"""
#         # 1. æœºå™¨äººçŠ¶æ€ï¼ˆ12ç»´ï¼‰
#         robot_obs = []
#         for name in self.robot_names:
#             robot_obs.extend([
#                 self.robot_positions[name].x / 200.0,
#                 self.robot_positions[name].y / 100.0,
#                 1.0 if self.robot_idle[name] else 0.0,
#                 min(self.robot_remaining_time[name] / 200.0, 1.0)
#             ])
        
#         # 2. ä»»åŠ¡çŠ¶æ€ï¼ˆ24ç»´ï¼š8ä¸ªä»»åŠ¡ï¼‰
#         task_obs = []
#         for _ in range(8):
#             task_obs.extend([0.0, 0.0, 0.0])
        
#         for i, task in enumerate(self.pending_tasks[:8]):
#             task_obs[i*3] = task["x"] / 200.0
#             task_obs[i*3+1] = task["y"] / 100.0
#             task_obs[i*3+2] = min(task["wait_time"] / 200.0, 1.0)
        
#         # 3. å…¨å±€çŠ¶æ€ï¼ˆ7ç»´ï¼Œåˆ é™¤äº†æ–°å¢çš„2ä¸ªç‰¹å¾ï¼‰
#         idle_robot_count = sum([1 for name in self.robot_names if self.robot_idle[name]])
#         robot_remaining_times = [min(self.robot_remaining_time[name]/200.0, 1.0) for name in self.robot_names]
#         global_obs = [
#             self.current_time / 1000.0,          # 1
#             len(self.completed_tasks) / 8.0,     # 2
#             len(self.pending_tasks) / 8.0,       # 3
#             idle_robot_count / 3.0,              # 4
#             # len(self.executing_tasks) / 8.0,   # <--- å·²åˆ é™¤
#             # sum(robot_remaining_times) / 3.0,  # <--- å·²åˆ é™¤
#             *robot_remaining_times               # 5-7ï¼ˆ3ä¸ªæœºå™¨äººå‰©ä½™æ—¶é—´ï¼‰
#         ]
        
#         # æ€»ç»´åº¦ï¼š12 + 24 + 7 = 43
#         return np.array(robot_obs + task_obs + global_obs, dtype=np.float32)

#     def step(self, action):
#         self.episode_steps += 1
#         self.current_time += 1.0
#         reward = self.reward_coeff["step"]
        
#         # å¥–åŠ±è£å‰ªå‡½æ•°
#         def clip_reward(r, min_r=-10.0, max_r=50.0):
#             return max(min(r, max_r), min_r)
        
#         wait_action_idx = len(self.robot_names)
#         info = {
#             "action_type": "wait" if action == wait_action_idx else "assign",
#             "selected_action": action,
#             "idle_robots": sum([1 for name in self.robot_names if self.robot_idle[name]])
#         }
        
#         # --- å¤„ç†ç­‰å¾…åŠ¨ä½œ ---
#         if action == wait_action_idx:
#             if self.pending_tasks:
#                 # æ›´æ–°ä»»åŠ¡ç­‰å¾…æ—¶é—´
#                 for task in self.pending_tasks:
#                     task["wait_time"] += 1.0
                
#                 # ç­‰å¾…å¥–åŠ±/æƒ©ç½š
#                 first_task_wait = self.pending_tasks[0]["wait_time"]
#                 if first_task_wait < 10.0:
#                     reward += self.reward_coeff["wait_short"]
#                     reward = clip_reward(reward)
#                 elif first_task_wait > 30.0:
#                     reward += self.reward_coeff["wait_long"]
#                     reward = clip_reward(reward)
                
#                 # è¶…æ—¶å¤„ç†ï¼šä»…æƒ©ç½šï¼Œä¸ä¸¢å¼ƒä»»åŠ¡
#                 idle_robot_count = sum([1 for name in self.robot_names if self.robot_idle[name]])
#                 if idle_robot_count > 0:
#                     for task in self.pending_tasks:
#                         tid = task["task_id"]
#                         if task["wait_time"] > 120.0 and self.task_timeout_count.get(tid, 0) == 0:
#                             reward += self.reward_coeff["timeout"]
#                             reward = clip_reward(reward)
#                             self.task_timeout_count[tid] = 1
        
#         # --- å¤„ç†é€‰è½¦åŠ¨ä½œ ---
#         else:
#             selected_robot = self.robot_names[action]
#             info["selected_robot"] = selected_robot
            
#             if not self.pending_tasks:
#                 truncated = self.episode_steps >= self.max_episode_steps
#                 return self._get_observation(), reward, False, truncated, info
            
#             # é€‰è·ç¦»æœ€è¿‘çš„ä»»åŠ¡
#             if self.robot_idle[selected_robot]:
#                 # é€‰ç©ºé—²è½¦çš„æ­£å‘å¥–åŠ±
#                 reward += self.reward_coeff["valid_selection"]
#                 reward = clip_reward(reward)
                
#                 min_dist = float("inf")
#                 best_task_idx = 0
#                 for i, task in enumerate(self.pending_tasks):
#                     rx = self.robot_positions[selected_robot].x
#                     ry = self.robot_positions[selected_robot].y
#                     dist = np.sqrt((rx - task["x"])**2 + (ry - task["y"])**2)
#                     if dist < min_dist:
#                         min_dist = dist
#                         best_task_idx = i
                
#                 task = self.pending_tasks.pop(best_task_idx)
#                 task_id = task["task_id"]
#                 info["task_id"] = task_id
                
#                 # æä½çš„è·ç¦»æƒ©ç½š
#                 reward += self.reward_coeff["distance"] * min_dist
#                 reward = clip_reward(reward)
                
#                 # è®¡ç®—å‰©ä½™æ‰§è¡Œæ—¶é—´
#                 self.robot_remaining_time[selected_robot] = dist / 1.0
                
#                 # æ›´æ–°çŠ¶æ€
#                 self.robot_idle[selected_robot] = False
#                 self.robot_task_map[selected_robot] = task_id
#                 self.executing_tasks[task_id] = {
#                     "robot": selected_robot,
#                     "start_time": self.current_time,
#                     "waypoint": task["waypoint"],
#                     "distance": min_dist
#                 }
#             else:
#                 # é€‰å¿™ç¢Œè½¦ï¼šæè½»æƒ©ç½š
#                 reward += self.reward_coeff["idle_selection"]
#                 reward = clip_reward(reward)
        
#         # --- å®Œæˆä»»åŠ¡å¥–åŠ± ---
#         self.total_reward += reward
#         truncated = self.episode_steps >= self.max_episode_steps
        
#         return self._get_observation(), reward, False, truncated, info

#     # è¾…åŠ©æ–¹æ³•ï¼šROS æ•°æ®æ›´æ–°
#     def update_robot_position(self, name, x, y):
#         if name in self.robot_positions:
#             self.robot_positions[name].x = x
#             self.robot_positions[name].y = y
            
#     def add_task(self, tid, wp, x, y):
#         # æ–°å¢ä»»åŠ¡æ—¶åˆå§‹åŒ–è¶…æ—¶è®¡æ•°
#         self.pending_tasks.append({
#             "task_id": tid, 
#             "waypoint": wp, 
#             "x": x, 
#             "y": y, 
#             "wait_time": 0.0
#         })
#         self.task_timeout_count[tid] = 0
        
#     def complete_task(self, tid):
#         if tid in self.executing_tasks:
#             robot = self.executing_tasks[tid]["robot"]
#             # ä»»åŠ¡å®Œæˆæ ¸å¿ƒå¥–åŠ±
#             self.total_reward += self.reward_coeff["completion"]
#             self.completed_tasks.append(tid)
#             # é‡ç½®å°è½¦çŠ¶æ€ï¼ˆåœåœ¨ä»»åŠ¡ç‚¹ï¼Œä¸è¿”å›å¾…å‘½ç‚¹ï¼‰
#             self.robot_idle[robot] = True
#             self.robot_remaining_time[robot] = 0.0
#             del self.executing_tasks[tid]
#             self.node.get_logger().info(f"ğŸ’° ä»»åŠ¡å®Œæˆå¥–åŠ± +{self.reward_coeff['completion']}! (Robot: {robot})")
            
#             # å®Œæˆæ‰€æœ‰8ä¸ªä»»åŠ¡ï¼šè¶…å¤§å¥–åŠ±
#             if len(self.completed_tasks) == 8:
#                 self.total_reward += self.reward_coeff["all_completed"]
#                 self.node.get_logger().info(f"ğŸ‰ å®Œæˆæ‰€æœ‰8ä¸ªä»»åŠ¡ï¼é¢å¤–å¥–åŠ± +{self.reward_coeff['all_completed']}")

# # ===================== æ ¸å¿ƒè°ƒåº¦èŠ‚ç‚¹ =====================
# class RLDispatcherNode(Node):
#     def __init__(self):
#         super().__init__("rl_dispatcher_node")
#         self.get_logger().info("ğŸš€ åˆå§‹åŒ– RL è°ƒåº¦èŠ‚ç‚¹ï¼ˆ8ä»»åŠ¡å¿…é¡»å®Œæˆç‰ˆï¼‰...")
        
#         # 1. ä»¿çœŸæ—¶é—´é…ç½®
#         if not self.has_parameter("use_sim_time"):
#             self.declare_parameter("use_sim_time", True)
#         self.set_parameters([rclpy.parameter.Parameter("use_sim_time", rclpy.Parameter.Type.BOOL, True)])

#         # 2. æ¨¡å¼é…ç½®
#         if not self.has_parameter("mode"):
#             self.declare_parameter("mode", "train")
#         self.mode = self.get_parameter("mode").get_parameter_value().string_value
#         self.get_logger().info(f"ğŸ”µ å½“å‰æ¨¡å¼: [{self.mode.upper()}]")
        
#         # 3. åŒå¼•æ“åˆ‡æ¢
#         if self.mode == "train":
#             self.get_logger().info("ğŸš€ è®­ç»ƒæ¨¡å¼ - æŒ‚è½½ Mock è™šæ‹Ÿå¼•æ“")
#             self.rl_env = MockRMFEnv() 
#         elif self.mode == "infer":
#             self.get_logger().info("ğŸ¤– æ¨ç†æ¨¡å¼ - æŒ‚è½½ Real çœŸå®å¼•æ“")
#             self.rl_env = RLDispatchingEnv(self, render_mode="human")
#         else:
#             self.get_logger().error("âŒ æœªçŸ¥æ¨¡å¼ï¼è¯·ä½¿ç”¨ mode:=train æˆ– mode:=infer")
#             sys.exit(1)

#         # 4. åˆå§‹åŒ– RL æ¨¡å‹ï¼ˆè°ƒä¼˜è¶…å‚æ•°ï¼Œè§£å†³è®­ç»ƒéœ‡è¡ï¼‰
#         self.model = None
#         self._init_rl_model()
        
#         # 5. æ¨ç†æ¨¡å¼åˆå§‹åŒ– ROS æ¥å£
#         if self.mode == "infer":
#             self._init_ros_interfaces()

#     def _init_ros_interfaces(self):
#         """åˆå§‹åŒ– ROS è®¢é˜…/å‘å¸ƒ/æœåŠ¡"""
#         self.get_logger().info("ğŸ”Œ è¿æ¥ ROS æ¥å£...")
        
#         # èˆªç‚¹åæ ‡æ˜ å°„
#         self.waypoint_coords = {
#             "n14": Point(x=80.84, y=-28.52), "n13": Point(x=84.44, y=-4.94),
#             "n23": Point(x=182.80, y=-42.30), "s08": Point(x=96.61, y=-50.50),
#             "s10": Point(x=122.10, y=-46.68), "west_koi_pond": Point(x=34.32, y=-10.13),
#             "n08": Point(x=59.61, y=-7.42), "junction_south_west": Point(x=84.56, y=-38.81)
#         }

#         # è®¢é˜…å™¨ï¼ˆä»…ä¿ç•™ä½ å·²éªŒè¯å¯ç”¨çš„ï¼‰
#         self.create_subscription(FleetState, "/fleet_states", self.fleet_state_callback, 10)
#         self.create_subscription(String, "/task_monitor/start", self.target_callback, 10)
#         self.create_subscription(String, "/custom_task_completion", self.completion_callback, 10)
        
#         # æœåŠ¡å®¢æˆ·ç«¯
#         self.nav_client = self.create_client(SingleNavTask, "/submit_single_nav_task")
        
#         # å†³ç­–å®šæ—¶å™¨ï¼ˆ1ç§’/æ¬¡ï¼‰
#         self.create_timer(1.0, self.infer_dispatch_callback)
        
#         # å»é‡ç¼“å­˜
#         self.processed_ids = set()
        
#         # æ–°å¢ï¼šè®°å½•æ­£åœ¨è·¯ä¸Šçš„è¯·æ±‚ï¼Œé˜²æ­¢é‡å¤æäº¤
#         self.dispatching_task_ids = set() 
        
#         self.get_logger().info("âœ… ROS æ¥å£è¿æ¥å®Œæˆ")

#     def _init_rl_model(self):
#         """åˆå§‹åŒ– PPO æ¨¡å‹ï¼ˆè°ƒä¼˜è¶…å‚æ•°ï¼Œè§£å†³KLæ•£åº¦è¿‡ä½ã€å¥–åŠ±éœ‡è¡é—®é¢˜ï¼‰"""
#         model_path = "./rl_models/ppo_800000_steps"
#         log_dir = "./rl_dispatching_tb_log/"
        
#         if self.mode == "train":
#             # è½»é‡åŒ–ç½‘ç»œï¼ˆé¿å…è¿‡æ‹Ÿåˆï¼‰
#             policy_kwargs = dict(
#                 activation_fn=th.nn.ReLU,
#                 net_arch=dict(pi=[128, 128], vf=[128, 128])
#             )
#             self.get_logger().info("ğŸ“š åˆå§‹åŒ– PPO æ¨¡å‹ï¼ˆè°ƒä¼˜è¶…å‚æ•°ç‰ˆï¼‰...")
#             self.model = PPO(
#                 "MlpPolicy", 
#                 self.rl_env, 
#                 verbose=1, 
#                 device='cpu', 
#                 tensorboard_log=log_dir,
#                 policy_kwargs=policy_kwargs,
#                 learning_rate=1e-4,        # ä»3e-5â†’1e-4ï¼šæé«˜å­¦ä¹ ç‡ï¼Œå¢å¼ºç­–ç•¥æ›´æ–°
#                 n_steps=2048,              # ä¿æŒä¸å˜
#                 batch_size=64,             # ä¿æŒä¸å˜
#                 gamma=0.99,                # ä¿æŒä¸å˜
#                 gae_lambda=0.95,           # ä¿æŒä¸å˜
#                 ent_coef=0.05,             # ä»0.01â†’0.05ï¼šæé«˜ç†µç³»æ•°ï¼Œå¢å¼ºæ¢ç´¢
#                 vf_coef=0.5,               # ä¿æŒä¸å˜
#                 max_grad_norm=0.5,         # ä¿æŒä¸å˜
#                 n_epochs=10,               # ä¿æŒä¸å˜
#                 clip_range=0.3             # ä»0.2â†’0.3ï¼šæ‰©å¤§è£å‰ªèŒƒå›´ï¼Œè§£å†³KLæ•£åº¦è¿‡ä½
#             )
#             # å›è°ƒå‡½æ•°ï¼šç›‘æ§æ”¶æ•›ï¼Œä¿å­˜æœ€ä¼˜æ¨¡å‹
#             self.checkpoint_callback = CheckpointCallback(save_freq=20000, save_path="./rl_models/", name_prefix="ppo")
            
#             # å¥–åŠ±é˜ˆå€¼åœæ­¢å›è°ƒï¼ˆä½œä¸ºEvalCallbackçš„å­å›è°ƒï¼‰
#             self.stop_callback = StopTrainingOnRewardThreshold(
#                 reward_threshold=20000.0,  # è®¾å®šåˆç†å¥–åŠ±é˜ˆå€¼
#                 verbose=1
#             )
            
#             # EvalCallbackï¼šå°†åœæ­¢å›è°ƒä½œä¸ºå­å›è°ƒä¼ å…¥
#             self.eval_callback = EvalCallback(
#                 self.rl_env,
#                 eval_freq=5000,
#                 n_eval_episodes=5,
#                 best_model_save_path="./rl_models/best/",
#                 verbose=1,
#                 callback_after_eval=self.stop_callback
#             )
            
#         elif self.mode == "infer":
#             if os.path.exists(model_path + ".zip"):
#                 self.get_logger().info(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {model_path}")
#                 self.model = PPO.load(model_path, device='cpu')
#             else:
#                 self.get_logger().error(f"âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}.zip")
#                 self.model = None

#     # è®­ç»ƒé€»è¾‘
#     def start_training(self, total_timesteps=1000000):
#         if self.mode != "train": return
            
#         self.get_logger().info(f"ğŸ”¥ å¼€å§‹è®­ç»ƒ {total_timesteps} æ­¥ï¼ˆ8ä»»åŠ¡å¿…é¡»å®Œæˆç‰ˆï¼‰...")
#         start_t = time.time()
        
#         self.model.learn(
#             total_timesteps=total_timesteps, 
#             callback=[self.checkpoint_callback, self.eval_callback]
#         )
        
#         # è¯„ä¼°æœ€ç»ˆæ¨¡å‹
#         mean_reward, std_reward = evaluate_policy(self.model, self.rl_env, n_eval_episodes=10)
#         self.get_logger().info(f"ğŸ“Š æœ€ç»ˆæ¨¡å‹è¯„ä¼°ï¼šå¹³å‡å¥–åŠ±={mean_reward:.2f}ï¼Œæ ‡å‡†å·®={std_reward:.2f}")
        
#         self.model.save("./rl_models/dispatching_ppo_final")
#         self.get_logger().info("ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ï¼")
#         rclpy.shutdown()

#     # æ¨ç†é€»è¾‘ï¼ˆæ ¸å¿ƒï¼šè¿‡æ»¤å·²æ‰§è¡Œ/æ­£åœ¨æäº¤çš„ä»»åŠ¡ï¼Œé˜²æ­¢é‡å¤ï¼‰
#     def infer_dispatch_callback(self):
#         # è¿‡æ»¤æ¡ä»¶ï¼šä»…ä¿ç•™æœªæ‰§è¡Œã€æœªæäº¤çš„red_cubeä»»åŠ¡
#         valid_tasks = [
#             t for t in self.rl_env.pending_tasks 
#             if t["task_id"].startswith("red_cube_") 
#             and t["task_id"] not in self.rl_env.executing_tasks
#             and t["task_id"] not in self.dispatching_task_ids
#         ]
#         if not valid_tasks:
#             self.get_logger().debug("ğŸ“­ æ— æœ‰æ•ˆä»»åŠ¡ï¼Œè·³è¿‡è°ƒåº¦")
#             return
        
#         # æ¨¡å‹å†³ç­–
#         obs = self.rl_env._get_observation()
#         action = 0
#         if self.model:
#             action, _ = self.model.predict(obs, deterministic=True)
        
#         wait_action_idx = len(self.rl_env.robot_names)
        
#         # å¤„ç†ç­‰å¾…åŠ¨ä½œ
#         if action == wait_action_idx:
#             self.get_logger().info("â³ æ¨¡å‹å†³ç­–ï¼šç­‰å¾…æ–°ä»»åŠ¡ï¼ˆå‡‘å•åæ‰¹é‡è°ƒåº¦ï¼‰")
#             for task in self.rl_env.pending_tasks:
#                 task["wait_time"] += 1.0
#             return
        
#         # å¤„ç†é€‰è½¦åŠ¨ä½œ
#         selected_robot = self.rl_env.robot_names[action]
        
#         # å®‰å…¨é™çº§ï¼šé€‰å¿™ç¢Œè½¦åˆ™åˆ‡æ¢åˆ°ç©ºé—²è½¦
#         if not self.rl_env.robot_idle[selected_robot]:
#             idle_indices = [i for i, name in enumerate(self.rl_env.robot_names) if self.rl_env.robot_idle[name]]
#             if idle_indices:
#                 action = random.choice(idle_indices)
#                 selected_robot = self.rl_env.robot_names[action]
#                 self.get_logger().warn(f"ğŸ›¡ï¸ ä¿®æ­£ï¼šé€‰ç©ºé—²è½¦ {selected_robot}")
        
#         # é€‰è·ç¦»æœ€è¿‘çš„ä»»åŠ¡
#         min_dist = float("inf")
#         best_task = None
#         for task in valid_tasks:
#             rx = self.rl_env.robot_positions[selected_robot].x
#             ry = self.rl_env.robot_positions[selected_robot].y
#             dist = np.sqrt((rx - task["x"])**2 + (ry - task["y"])**2)
#             if dist < min_dist:
#                 min_dist = dist
#                 best_task = task
        
#         if best_task:
#             tid = best_task["task_id"]
            
#             # é˜²æŠ–ï¼šæ­£åœ¨æäº¤çš„ä»»åŠ¡è·³è¿‡
#             if tid in self.dispatching_task_ids:
#                 self.get_logger().debug(f"â³ ä»»åŠ¡ {tid} æ­£åœ¨è¯·æ±‚ä¸­ï¼Œè·³è¿‡...")
#                 return

#             # æ ‡è®°ä¸ºæ­£åœ¨æäº¤
#             self.dispatching_task_ids.add(tid)
            
#             # è°ƒç”¨æœåŠ¡ä¸‹å‘ä»»åŠ¡
#             self._call_ros_service(selected_robot, tid, best_task["waypoint"])

#     def _call_ros_service(self, robot, tid, wp):
#         if not self.nav_client.service_is_ready():
#             self.get_logger().warn("âš ï¸ æœåŠ¡æœªå°±ç»ªï¼Œè·³è¿‡ä»»åŠ¡ä¸‹å‘")
#             # ç§»é™¤æ ‡è®°ï¼Œé¿å…æ°¸ä¹…é”å®š
#             self.dispatching_task_ids.discard(tid)
#             return
            
#         req = SingleNavTask.Request()
#         req.target_waypoint = wp
#         req.fleet_name = "deliveryRobot"
#         req.robot_name = robot
#         req.priority = 1
        
#         future = self.nav_client.call_async(req)
#         future.add_done_callback(lambda f: self._service_done(f, tid, robot))

#     def _service_done(self, future, tid, robot):
#         """ã€ä¿®æ”¹ç‚¹3ã€‘ä¿®å¤ï¼šæ— è®ºæˆåŠŸå¤±è´¥ï¼Œåªè¦æœ‰ç»“æœäº†ï¼Œå°±ä»â€œå‘é€ä¸­â€åå•ç§»é™¤ï¼Œå¹¶é˜²æ­¢å­—ç¬¦ä¸²åˆ‡å‰²æŠ¥é”™"""
#         self.dispatching_task_ids.discard(tid)

#         try:
#             res = future.result()
#             if res.success:
#                 self.get_logger().info(f"ğŸš€ è°ƒåº¦æˆåŠŸï¼š{robot} -> {tid} @ {res.message}")
                
#                 # éå† pending_tasks æ‰¾åˆ°å¯¹åº”çš„ä»»åŠ¡å¯¹è±¡
#                 target_task_idx = -1
#                 for i, t in enumerate(self.rl_env.pending_tasks):
#                     if t["task_id"] == tid:
#                         target_task_idx = i
#                         break
                
#                 if target_task_idx != -1:
#                     # å–å‡ºä»»åŠ¡æ•°æ®
#                     task_data = self.rl_env.pending_tasks.pop(target_task_idx)
                    
#                     # æ›´æ–°çŠ¶æ€
#                     self.rl_env.robot_idle[robot] = False
#                     self.rl_env.robot_task_map[robot] = tid
                    
#                     # ã€æ ¸å¿ƒä¿®å¤ã€‘ç›´æ¥ä½¿ç”¨ task_data ä¸­çš„ waypointï¼Œä¸è¦ split ID
#                     wp_name = task_data["waypoint"] 
                    
#                     # è®¡ç®—è·ç¦»
#                     if wp_name in self.waypoint_coords:
#                         target_point = self.waypoint_coords[wp_name]
#                         current_pos = self.rl_env.robot_positions[robot]
#                         dist = np.sqrt((current_pos.x - target_point.x)**2 + (current_pos.y - target_point.y)**2)
#                     else:
#                         dist = 0.0 # å…œåº•

#                     self.rl_env.executing_tasks[tid] = {
#                         "robot": robot,
#                         "start_time": self.rl_env.current_time,
#                         "waypoint": self.waypoint_coords.get(wp_name, Point()), # é˜²æ­¢ç©ºæŒ‡é’ˆ
#                         "distance": dist
#                     }
#             else:
#                 self.get_logger().error(f"âŒ è°ƒåº¦å¤±è´¥ï¼š{tid} -> {res.message}")
#         except Exception as e:
#             self.get_logger().error(f"âŒ æœåŠ¡è°ƒç”¨å¼‚å¸¸ï¼š{e}")
#             import traceback
#             traceback.print_exc()

#     def fleet_state_callback(self, msg):
#         """æ ¸å¿ƒä¿®å¤ï¼šåŸºäºä½ å®é™…çš„ RobotState å­—æ®µåˆ¤æ–­ç©ºé—²çŠ¶æ€"""
#         if msg.name != "deliveryRobot": return
#         for r in msg.robots:
#             if r.name in self.rl_env.robot_names:
#                 # 1. æ›´æ–°å°è½¦ä½ç½®ï¼ˆä¿ç•™ï¼‰
#                 self.rl_env.update_robot_position(r.name, r.location.x, r.location.y)
                
#                 # 2. ä¿®å¤ï¼šåŸºäºå®é™…å­—æ®µåˆ¤æ–­ç©ºé—²çŠ¶æ€ï¼ˆæ— task_stateï¼Œç”¨mode+task_idï¼‰
#                 # RMF RobotModeæšä¸¾å€¼ï¼š1=IDLE, 2=MOVING, 3=PAUSED, 4=CHARGING
#                 is_idle = (r.mode.mode == RobotMode.MODE_IDLE) and (not r.task_id) and (r.battery_percent > 0.0)
#                 self.rl_env.robot_idle[r.name] = is_idle
                
#                 # 3. åŒæ­¥å‰©ä½™æ‰§è¡Œæ—¶é—´ï¼ˆä¿ç•™ï¼‰
#                 if not is_idle and r.task_id:
#                     if r.name in self.rl_env.robot_task_map:
#                         tid = self.rl_env.robot_task_map[r.name]
#                         if tid in self.rl_env.executing_tasks:
#                             wp = self.rl_env.executing_tasks[tid]["waypoint"]
#                             current_pos = Point(x=r.location.x, y=r.location.y)
#                             dist = np.sqrt((current_pos.x - wp.x)**2 + (current_pos.y - wp.y)**2)
#                             self.rl_env.robot_remaining_time[r.name] = dist / 1.0
#                 else:
#                     self.rl_env.robot_remaining_time[r.name] = 0.0

#     def target_callback(self, msg):
#         """å¤„ç†ä»»åŠ¡å‘å¸ƒè¯é¢˜ï¼Œæ–°å¢ä»»åŠ¡åˆ°å¾…æ‰§è¡Œé˜Ÿåˆ—"""
#         data = msg.data.split(",")
#         if len(data) < 2:
#             self.get_logger().warn(f"âš ï¸ æ— æ•ˆä»»åŠ¡æ ¼å¼ï¼š{msg.data}")
#             return
        
#         tid, wp = data[0].strip(), data[1].strip()
#         # å»é‡ï¼šå·²å¤„ç†çš„ä»»åŠ¡ä¸å†æ·»åŠ 
#         if not tid.startswith("red_cube_") or tid in self.processed_ids:
#             return
        
#         if wp in self.waypoint_coords:
#             p = self.waypoint_coords[wp]
#             self.rl_env.add_task(tid, wp, p.x, p.y)
#             self.processed_ids.add(tid)
#             self.get_logger().info(f"ğŸ“¥ æ–°å¢ä»»åŠ¡ï¼š{tid} @ {wp}")
#         else:
#             self.get_logger().error(f"âŒ æœªçŸ¥èˆªç‚¹ï¼š{wp}ï¼Œä»»åŠ¡{tid}æ·»åŠ å¤±è´¥")

#     def completion_callback(self, msg):
#         """å¤„ç†ä»»åŠ¡å®Œæˆè¯é¢˜ï¼Œé‡ç½®å°è½¦çŠ¶æ€"""
#         data = msg.data.split(",")
#         if len(data) < 1:
#             self.get_logger().warn(f"âš ï¸ æ— æ•ˆçš„å®Œæˆæ¶ˆæ¯ï¼š{msg.data}")
#             return
#         tid = data[0].strip()
        
#         if tid.startswith("red_cube_"):
#             # è°ƒç”¨ç¯å¢ƒçš„å®Œæˆä»»åŠ¡æ–¹æ³•
#             self.rl_env.complete_task(tid)
#             # æ¸…ç†ç¼“å­˜
#             if tid in self.processed_ids:
#                 self.processed_ids.remove(tid)
#             # å¼ºåˆ¶é‡ç½®å¯¹åº”å°è½¦çŠ¶æ€ï¼ˆå…œåº•ï¼‰
#             for robot in self.rl_env.robot_names:
#                 if self.rl_env.robot_task_map.get(robot, "") == tid:
#                     self.rl_env.robot_idle[robot] = True
#                     self.rl_env.robot_remaining_time[robot] = 0.0
#                     del self.rl_env.robot_task_map[robot]
#                     break

# # ä¸»ç¨‹åº
# def main(args=None):
#     rclpy.init(args=args)
#     node = RLDispatcherNode()
    
#     if node.mode == "train":
#         train_thread = threading.Thread(
#             target=node.start_training, 
#             args=(1000000,),
#             daemon=True
#         )
#         train_thread.start()
#         try:
#             rclpy.spin(node)
#         except SystemExit:
#             pass
#         train_thread.join()
#     else:
#         try:
#             rclpy.spin(node)
#         except KeyboardInterrupt:
#             pass
        
#     node.destroy_node()
#     if rclpy.ok():
#         rclpy.shutdown()

# if __name__ == "__main__":
#     main()

#!/usr/bin/env python3
#1.20ï¼Œzjtæ”¹
# é€‚é…éœ€æ±‚ï¼š8ä»»åŠ¡å¿…é¡»å®Œæˆ+æ— å¾…å‘½ç‚¹è¿”å›+è¶…æ—¶ä¸ä¸¢ä»»åŠ¡
# æ ¸å¿ƒä¼˜åŒ–ï¼šä¸‹è°ƒå¥–åŠ±ç³»æ•°ã€è°ƒä¼˜PPOè¶…å‚æ•°ï¼Œè§£å†³è®­ç»ƒéœ‡è¡é—®é¢˜
