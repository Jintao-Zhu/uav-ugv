#!/usr/bin/env python3
# é€‚é…éœ€æ±‚ï¼š8ä»»åŠ¡å¿…é¡»å®Œæˆ+æ— å¾…å‘½ç‚¹è¿”å›+è¶…æ—¶ä¸ä¸¢ä»»åŠ¡
# æ ¸å¿ƒä¼˜åŒ–ï¼šä¿®å¤ä»»åŠ¡é›†ä¸­åˆ†é…+å¥–åŠ±è¿‡å¤§+æ ‡å‡†å·®é«˜é—®é¢˜
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import rclpy
from rclpy.node import Node
from rmf_fleet_msgs.msg import FleetState
from std_msgs.msg import String
from geometry_msgs.msg import Point
from rmf_custom_tasks_self.srv import SingleNavTask
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, StopTrainingOnRewardThreshold, BaseCallback
from stable_baselines3.common.evaluation import evaluate_policy
import time
import random
import threading
import torch as th
from stable_baselines3.common.monitor import Monitor  # æ–°å¢ï¼šè§£å†³éªŒè¯ç¯å¢ƒå¥–åŠ±ç»Ÿè®¡é”™è¯¯

# ========== ä¿®å¤æ ¸å¿ƒï¼šæ·»åŠ ç¼ºå¤±çš„ gymnasium å¯¼å…¥ ==========
import gymnasium as gym
from gymnasium import spaces

# å¼•å…¥æ¨¡æ‹Ÿç¯å¢ƒ
try:
    from mock_env import MockRMFEnv
except ImportError:
    print("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° mock_env.pyï¼Œè¯·ç¡®ä¿æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼")
    sys.exit(1)

# ===================== çœŸå® ROS ç¯å¢ƒ =====================
class RLDispatchingEnv(gym.Env):
    """
    çœŸå®RMFç¯å¢ƒï¼šå®Œå…¨å¯¹é½Mocké€»è¾‘ï¼Œæ”¯æŒä»»åŠ¡æ’é˜Ÿ+æ— å¾…å‘½ç‚¹è¿”å›
    ä¼˜åŒ–ï¼šæ–°å¢é˜Ÿåˆ—å‡è¡¡æƒ©ç½š+å¥–åŠ±å½’ä¸€åŒ–+çŠ¶æ€ç‰¹å¾ä¼˜åŒ–
    """
    metadata = {"render_modes": ["human"], "render_fps": 1}
    
    def __init__(self, node, render_mode=None):
        super().__init__()
        self.node = node
        self.render_mode = render_mode
        
        # 1. å°è½¦é…ç½®ï¼ˆæ”¯æŒä»»åŠ¡æ’é˜Ÿï¼‰
        self.robot_names = ["deliveryRobot_0", "deliveryRobot_1", "deliveryRobot_2"]
        self.robot_positions = {
            "deliveryRobot_0": Point(x=96.59527587890625, y=-51.96450424194336),
            "deliveryRobot_1": Point(x=152.3477325439453, y=-44.31863021850586),
            "deliveryRobot_2": Point(x=14.776845932006836, y=-9.279278755187988)
        }
        # å‡çº§ï¼šæ”¯æŒä»»åŠ¡é˜Ÿåˆ—
        self.robot_states = {
            name: {
                "idle": True,
                "current_target": None,
                "task_queue": [],
                "current_task_remaining_time": 0.0
            } for name in self.robot_names
        }
        
        # 2. ä»»åŠ¡é˜Ÿåˆ—ï¼ˆä¿ç•™æ‰€æœ‰ä»»åŠ¡ï¼Œä¸ä¸¢å¼ƒï¼‰
        self.pending_tasks = []
        self.completed_tasks = []
        self.task_timeout_count = {}  # è®°å½•ä»»åŠ¡è¶…æ—¶æ¬¡æ•°ï¼Œé¿å…é‡å¤æƒ©ç½š
        
        # 3. ç»Ÿè®¡å˜é‡
        self.current_time = 0.0
        self.episode_steps = 0
        self.max_episode_steps = 1000
        self.total_reward = 0.0
        
        # 4. çŠ¶æ€ç©ºé—´ï¼ˆä¸Mockå®Œå…¨å¯¹é½ï¼š45ç»´ï¼‰
        self.action_space = spaces.Discrete(len(self.robot_names) + 1)
        self.observation_space = spaces.Box(low=-1.0, high=1.0, shape=(45,), dtype=np.float32)
        
        # 5. å¥–åŠ±ç³»æ•°ï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼šå½’ä¸€åŒ–+æ–°å¢é˜Ÿåˆ—å‡è¡¡æƒ©ç½šï¼‰
        self.reward_coeff = {
            "distance": -0.001,         
            "completion": 1.0,          
            "batch_completion": 1.5,    
            "invalid_selection": -0.1,  
            "timeout": -0.1,            
            "step": -0.001,             
            "wait_short": 0.001,         
            "wait_long": -0.01,        
            "all_completed": 5.0,      
            "task_queue": 0.01,         
            "queue_imbalance": -0.001    
        }

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        # é‡ç½®å°è½¦çŠ¶æ€ï¼ˆæ”¯æŒä»»åŠ¡é˜Ÿåˆ—ï¼‰
        for name in self.robot_names:
            self.robot_states[name] = {
                "idle": True,
                "current_target": None,
                "task_queue": [],
                "current_task_remaining_time": 0.0
            }
        self.pending_tasks = []
        self.completed_tasks = []
        self.task_timeout_count = {}
        self.current_time = 0.0
        self.episode_steps = 0
        self.total_reward = 0.0
        return self._get_observation(), {"total_reward": 0.0}

    def _get_observation(self):
        """45ç»´çŠ¶æ€å‘é‡ï¼šä¼˜åŒ–å…¨å±€ç‰¹å¾ï¼Œç§»é™¤å†—ä½™ï¼Œæ–°å¢é˜Ÿåˆ—å‡è¡¡ç‰¹å¾"""
        # 1. æœºå™¨äººçŠ¶æ€ï¼ˆ12ç»´ï¼šx, y, idle, é˜Ÿåˆ—é•¿åº¦ï¼‰
        robot_obs = []
        for name in self.robot_names:
            r = self.robot_states[name]
            pos = self.robot_positions[name]
            queue_len = len(r["task_queue"]) if not r["idle"] else 0.0
            robot_obs.extend([
                pos.x / 200.0,
                pos.y / 100.0,
                1.0 if r["idle"] else 0.0,
                min(queue_len / 5.0, 1.0)  # é˜Ÿåˆ—é•¿åº¦å½’ä¸€åŒ–
            ])
        
        # 2. ä»»åŠ¡çŠ¶æ€ï¼ˆ24ç»´ï¼š8ä¸ªä»»åŠ¡ï¼‰
        task_obs = []
        for _ in range(8):
            task_obs.extend([0.0, 0.0, 0.0])
        
        for i, task in enumerate(self.pending_tasks[:8]):
            task_obs[i*3] = task["x"] / 200.0
            task_obs[i*3+1] = task["y"] / 100.0
            task_obs[i*3+2] = min(task["wait_time"] / 600.0, 1.0)  # ä¿®æ­£ç­‰å¾…æ—¶é—´å½’ä¸€åŒ–
        
        # 3. å…¨å±€çŠ¶æ€ï¼ˆ9ç»´ï¼šä¼˜åŒ–ç‰¹å¾ï¼Œç§»é™¤å†—ä½™ï¼‰
        idle_robot_count = sum([1 for name in self.robot_names if self.robot_states[name]["idle"]])
        total_queue_len = sum([len(self.robot_states[name]["task_queue"]) for name in self.robot_names])
        queue_lengths = [len(self.robot_states[name]["task_queue"]) for name in self.robot_names]
        max_queue_len = max(queue_lengths) if queue_lengths else 0
        min_queue_len = min(queue_lengths) if queue_lengths else 0
        
        global_obs = [
            self.current_time / 1000.0,          # 1. å½“å‰æ—¶é—´
            len(self.completed_tasks) / 8.0,     # 2. å·²å®Œæˆä»»åŠ¡è¿›åº¦
            len(self.pending_tasks) / 8.0,       # 3. å¾…å¤„ç†ä»»åŠ¡è¿›åº¦
            idle_robot_count / 3.0,              # 4. ç©ºé—²æœºå™¨äººå æ¯”
            total_queue_len / 15.0,              # 5. æ€»é˜Ÿåˆ—é•¿åº¦å æ¯”
            (len(self.completed_tasks) + len(self.pending_tasks)) / 8.0,  # 6. æ€»ä»»åŠ¡è¿›åº¦
            # ä¼˜åŒ–ï¼šæ–°å¢é˜Ÿåˆ—å‡è¡¡ç‰¹å¾ï¼ˆç§»é™¤å†—ä½™çš„å•å°é˜Ÿåˆ—é•¿åº¦ï¼‰
            np.var(queue_lengths) / 2.5 if queue_lengths else 0.0,        # 7. é˜Ÿåˆ—æ–¹å·®ï¼ˆå‡è¡¡æ€§ï¼‰
            max_queue_len / 5.0,                                          # 8. æœ€é•¿é˜Ÿåˆ—ï¼ˆå‹åŠ›å³°å€¼ï¼‰
            (max_queue_len - min_queue_len) / 5.0 if queue_lengths else 0.0,  # 9. é˜Ÿåˆ—æœ€å¤§å·®ï¼ˆä¸å‡è¡¡åº¦ï¼‰
        ]
        
        # å…œåº•ï¼šç¡®ä¿æ‰€æœ‰ç»´åº¦åœ¨[-1,1]èŒƒå›´å†…
        global_obs = [min(max(x, -1.0), 1.0) for x in global_obs]
        
        return np.array(robot_obs + task_obs + global_obs, dtype=np.float32)

    def step(self, action):
        self.episode_steps += 1
        self.current_time += 1.0
        reward_items = []  # æ”¶é›†æ‰€æœ‰å¥–åŠ±é¡¹ï¼Œæœ€åç»Ÿä¸€è®¡ç®—
        reward_items.append(self.reward_coeff["step"])  # åŸºç¡€æ—¶é—´æƒ©ç½š

        # å¥–åŠ±è£å‰ªå‡½æ•°
        def clip_reward(r, min_r=-10.0, max_r=50.0):
            return max(min(r, max_r), min_r)
        
        wait_action_idx = len(self.robot_names)
        info = {
            "action_type": "wait" if action == wait_action_idx else "assign",
            "selected_action": action,
            "idle_robots": sum([1 for name in self.robot_names if self.robot_states[name]["idle"]]),
            "robot_task_queues": {name: len(self.robot_states[name]["task_queue"]) for name in self.robot_names}
        }
        
        # --- å¤„ç†ç­‰å¾…åŠ¨ä½œ ---
        if action == wait_action_idx:
            if self.pending_tasks:
                # æ›´æ–°ä»»åŠ¡ç­‰å¾…æ—¶é—´
                for task in self.pending_tasks:
                    task["wait_time"] += 1.0
                
                # ç­‰å¾…å¥–åŠ±/æƒ©ç½š
                first_task_wait = self.pending_tasks[0]["wait_time"]
                if first_task_wait < 10.0:
                    reward_items.append(self.reward_coeff["wait_short"])
                elif first_task_wait > 30.0:
                    reward_items.append(self.reward_coeff["wait_long"])
                
                # è¶…æ—¶å¤„ç†ï¼šä»…æƒ©ç½šï¼Œä¸ä¸¢å¼ƒä»»åŠ¡
                for task in self.pending_tasks:
                    tid = task["task_id"]
                    if task["wait_time"] > 120.0 and self.task_timeout_count.get(tid, 0) == 0:
                        reward_items.append(self.reward_coeff["timeout"])
                        self.task_timeout_count[tid] = 1
        
        # --- å¤„ç†é€‰è½¦åŠ¨ä½œï¼ˆæ”¯æŒä»»åŠ¡æ’é˜Ÿï¼‰ ---
        else:
            selected_robot = self.robot_names[action]
            info["selected_robot"] = selected_robot
            robot = self.robot_states[selected_robot]
            
            # æ— ä»»åŠ¡æ—¶é€‰è½¦ï¼šæ— æ•ˆæ“ä½œï¼Œæƒ©ç½š
            if not self.pending_tasks:
                reward_items.append(self.reward_coeff["invalid_selection"])
                reward = sum(reward_items)
                reward = clip_reward(reward)
                truncated = self.episode_steps >= self.max_episode_steps
                return self._get_observation(), reward, False, truncated, info
            
            # é€‰è·ç¦»æœ€è¿‘çš„ä»»åŠ¡
            min_dist = float("inf")
            best_task_idx = 0
            for i, task in enumerate(self.pending_tasks):
                rx = self.robot_positions[selected_robot].x
                ry = self.robot_positions[selected_robot].y
                dist = np.sqrt((rx - task["x"])**2 + (ry - task["y"])**2)
                if dist < min_dist:
                    min_dist = dist
                    best_task_idx = i
            
            task = self.pending_tasks.pop(best_task_idx)
            task_id = task["task_id"]
            info["task_id"] = task_id
            
            # è·ç¦»æƒ©ç½šï¼ˆå¼•å¯¼è¿‘ä»»åŠ¡ä¼˜å…ˆï¼‰
            reward_items.append(self.reward_coeff["distance"] * min_dist)
            
            # è®¡ç®—ä»»åŠ¡æ‰§è¡Œæ—¶é—´
            task_exec_time = min_dist / 1.0
            task_info = {
                "task_id": task_id,
                "waypoint": task["waypoint"],
                "x": task["x"],
                "y": task["y"],
                "exec_time": task_exec_time
            }
            
            # æ ¸å¿ƒé€»è¾‘ï¼šä»»åŠ¡æ’é˜Ÿï¼ˆæ— é€‰è½¦æƒ©ç½šï¼‰
            if robot["idle"]:
                # å°è½¦ç©ºé—²ï¼šç›´æ¥æ‰§è¡Œè¯¥ä»»åŠ¡
                robot["idle"] = False
                robot["current_target"] = task_info
                robot["current_task_remaining_time"] = task_exec_time
            else:
                # å°è½¦å¿™ç¢Œï¼šåŠ å…¥ä»»åŠ¡é˜Ÿåˆ—ï¼ˆå¥–åŠ±åˆç†æ’é˜Ÿï¼‰
                robot["task_queue"].append(task_info)
                reward_items.append(self.reward_coeff["task_queue"])
        
        # --- æ¨¡æ‹Ÿä»»åŠ¡æ‰§è¡Œï¼ˆé€‚é…æ’é˜Ÿé€»è¾‘ï¼‰ ---
        task_completion_rewards = 0
        for name in self.robot_names:
            r = self.robot_states[name]
            # å°è½¦å¿™ç¢Œä¸”æœ‰å½“å‰ä»»åŠ¡
            if not r["idle"] and r["current_target"]:
                # æ›´æ–°å‰©ä½™æ‰§è¡Œæ—¶é—´
                r["current_task_remaining_time"] -= 1.0
                r["current_task_remaining_time"] = max(0.0, r["current_task_remaining_time"])
                
                # ä»»åŠ¡å®Œæˆ
                if r["current_task_remaining_time"] <= 0:
                    # ä»»åŠ¡å®Œæˆå¥–åŠ±
                    task_completion_rewards += self.reward_coeff["completion"]
                    self.completed_tasks.append(r["current_target"]["task_id"])
                    self.node.get_logger().info(f"ğŸ’° ä»»åŠ¡å®Œæˆå¥–åŠ± +{self.reward_coeff['completion']}! (Robot: {name})")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ’é˜Ÿä»»åŠ¡
                    if r["task_queue"]:
                        # æ‰§è¡Œä¸‹ä¸€ä¸ªæ’é˜Ÿä»»åŠ¡
                        next_task = r["task_queue"].pop(0)
                        r["current_target"] = next_task
                        r["current_task_remaining_time"] = next_task["exec_time"]
                    else:
                        # æ— æ’é˜Ÿä»»åŠ¡ï¼šå°è½¦ç©ºé—²
                        r["idle"] = True
                        r["current_target"] = None
                        r["current_task_remaining_time"] = 0.0
                    
                    # å®Œæˆæ‰€æœ‰8ä¸ªä»»åŠ¡ï¼šè¶…å¤§å¥–åŠ±
                    if len(self.completed_tasks) == 8:
                        task_completion_rewards += self.reward_coeff["all_completed"]
                        self.node.get_logger().info(f"ğŸ‰ å®Œæˆæ‰€æœ‰8ä¸ªä»»åŠ¡ï¼é¢å¤–å¥–åŠ± +{self.reward_coeff['all_completed']}")
        
        reward_items.append(task_completion_rewards)
        
        # --- æ‰¹é‡å®Œæˆå¥–åŠ± ---
        if len(self.pending_tasks) == 0 and len(self.completed_tasks) > 0:
            reward_items.append(self.reward_coeff["batch_completion"])
        
        # --- é˜Ÿåˆ—ä¸å‡è¡¡æƒ©ç½š ---
        queue_lengths = [len(self.robot_states[name]["task_queue"]) for name in self.robot_names]
        if queue_lengths:
            max_queue = max(queue_lengths)
            avg_queue = np.mean(queue_lengths)
            if max_queue > avg_queue + 2:
                reward_items.append(self.reward_coeff["queue_imbalance"] * (max_queue - avg_queue))
        
        # --- è®¡ç®—æ€»å¥–åŠ±å¹¶è£å‰ª ---
        reward = sum(reward_items)
        reward = clip_reward(reward)
        
        # --- ç»Ÿè®¡ä¸ç»“æŸæ¡ä»¶ ---
        self.total_reward += reward
        truncated = self.episode_steps >= self.max_episode_steps
        # ç»“æŸæ¡ä»¶ï¼šä»»åŠ¡å®Œæˆ+æ— æ’é˜Ÿä»»åŠ¡
        terminated = (len(self.pending_tasks) == 0 and 
                      len(self.completed_tasks) == 8 and
                      all([r["idle"] and len(r["task_queue"]) == 0 for r in self.robot_states.values()]))
        
        return self._get_observation(), reward, terminated, truncated, info

    # è¾…åŠ©æ–¹æ³•ï¼šROS æ•°æ®æ›´æ–°
    def update_robot_position(self, name, x, y):
        if name in self.robot_positions:
            self.robot_positions[name].x = x
            self.robot_positions[name].y = y
            
    def add_task(self, tid, wp, x, y):
        # æ–°å¢ä»»åŠ¡æ—¶åˆå§‹åŒ–è¶…æ—¶è®¡æ•°
        self.pending_tasks.append({
            "task_id": tid, 
            "waypoint": wp, 
            "x": x, 
            "y": y, 
            "wait_time": 0.0
        })
        self.task_timeout_count[tid] = 0
        
    def complete_task(self, tid):
        """é€‚é…RMFçœŸå®å›è°ƒçš„ä»»åŠ¡å®Œæˆé€»è¾‘"""
        # æŸ¥æ‰¾æ‰§è¡Œè¯¥ä»»åŠ¡çš„å°è½¦
        target_robot = None
        for name in self.robot_names:
            r = self.robot_states[name]
            # æ£€æŸ¥å½“å‰æ‰§è¡Œçš„ä»»åŠ¡
            if not r["idle"] and r["current_target"] and r["current_target"]["task_id"] == tid:
                target_robot = name
                break
            # æ£€æŸ¥æ’é˜Ÿä»»åŠ¡
            for i, task in enumerate(r["task_queue"]):
                if task["task_id"] == tid:
                    # ä»é˜Ÿåˆ—ä¸­ç§»é™¤ï¼ˆRMFä¸­å–æ¶ˆæ’é˜Ÿä»»åŠ¡çš„åœºæ™¯ï¼‰
                    r["task_queue"].pop(i)
                    self.node.get_logger().info(f"ğŸ“¤ ä»»åŠ¡ {tid} ä» {name} çš„æ’é˜Ÿé˜Ÿåˆ—ä¸­ç§»é™¤")
                    return
        
        if target_robot:
            r = self.robot_states[target_robot]
            # ä»»åŠ¡å®Œæˆå¥–åŠ±
            self.total_reward += self.reward_coeff["completion"]
            self.completed_tasks.append(tid)
            self.node.get_logger().info(f"ğŸ’° ä»»åŠ¡å®Œæˆå¥–åŠ± +{self.reward_coeff['completion']}! (Robot: {target_robot})")
            
            # æ‰§è¡Œä¸‹ä¸€ä¸ªæ’é˜Ÿä»»åŠ¡
            if r["task_queue"]:
                next_task = r["task_queue"].pop(0)
                r["current_target"] = next_task
                r["current_task_remaining_time"] = next_task["exec_time"]
            else:
                # æ— æ’é˜Ÿä»»åŠ¡ï¼šå°è½¦ç©ºé—²
                r["idle"] = True
                r["current_target"] = None
                r["current_task_remaining_time"] = 0.0
            
            # å®Œæˆæ‰€æœ‰8ä¸ªä»»åŠ¡ï¼šè¶…å¤§å¥–åŠ±
            if len(self.completed_tasks) == 8:
                self.total_reward += self.reward_coeff["all_completed"]
                self.node.get_logger().info(f"ğŸ‰ å®Œæˆæ‰€æœ‰8ä¸ªä»»åŠ¡ï¼é¢å¤–å¥–åŠ± +{self.reward_coeff['all_completed']}")

# ===================== æ ¸å¿ƒè°ƒåº¦èŠ‚ç‚¹ =====================
class RLDispatcherNode(Node):
    def __init__(self):
        super().__init__("rl_dispatcher_node")
        self.get_logger().info("ğŸš€ åˆå§‹åŒ– RL è°ƒåº¦èŠ‚ç‚¹ï¼ˆ8ä»»åŠ¡å¿…é¡»å®Œæˆç‰ˆï¼‰...")
        
        # 1. ä»¿çœŸæ—¶é—´é…ç½®
        if not self.has_parameter("use_sim_time"):
            self.declare_parameter("use_sim_time", True)
        self.set_parameters([rclpy.parameter.Parameter("use_sim_time", rclpy.Parameter.Type.BOOL, True)])

        # 2. æ¨¡å¼é…ç½®
        if not self.has_parameter("mode"):
            self.declare_parameter("mode", "train")
        self.mode = self.get_parameter("mode").get_parameter_value().string_value
        self.get_logger().info(f"ğŸ”µ å½“å‰æ¨¡å¼: [{self.mode.upper()}]")
        
        # 3. åŒå¼•æ“åˆ‡æ¢
        if self.mode == "train":
            self.get_logger().info("ğŸš€ è®­ç»ƒæ¨¡å¼ - æŒ‚è½½ Mock è™šæ‹Ÿå¼•æ“")
            self.rl_env = MockRMFEnv() 
        elif self.mode == "infer":
            self.get_logger().info("ğŸ¤– æ¨ç†æ¨¡å¼ - æŒ‚è½½ Real çœŸå®å¼•æ“")
            self.rl_env = RLDispatchingEnv(self, render_mode="human")
        else:
            self.get_logger().error("âŒ æœªçŸ¥æ¨¡å¼ï¼è¯·ä½¿ç”¨ mode:=train æˆ– mode:=infer")
            sys.exit(1)

        # 4. åˆå§‹åŒ– RL æ¨¡å‹ï¼ˆè°ƒä¼˜è¶…å‚æ•°ï¼Œå¢å¼ºæ¢ç´¢+é™ä½å¥–åŠ±éœ‡è¡ï¼‰
        self.model = None
        self._init_rl_model()
        
        # 5. æ¨ç†æ¨¡å¼åˆå§‹åŒ– ROS æ¥å£
        if self.mode == "infer":
            self._init_ros_interfaces()

    def _init_ros_interfaces(self):
        """åˆå§‹åŒ– ROS è®¢é˜…/å‘å¸ƒ/æœåŠ¡"""
        self.get_logger().info("ğŸ”Œ è¿æ¥ ROS æ¥å£...")
        
        # èˆªç‚¹åæ ‡æ˜ å°„
        self.waypoint_coords = {
            "n14": Point(x=80.84, y=-28.52), "n13": Point(x=84.44, y=-4.94),
            "n23": Point(x=182.80, y=-42.30), "s08": Point(x=96.61, y=-50.50),
            "s10": Point(x=122.10, y=-46.68), "west_koi_pond": Point(x=34.32, y=-10.13),
            "n08": Point(x=59.61, y=-7.42), "junction_south_west": Point(x=84.56, y=-38.81)
        }

        # è®¢é˜…å™¨
        self.create_subscription(FleetState, "/fleet_states", self.fleet_state_callback, 10)
        self.create_subscription(String, "/task_monitor/start", self.target_callback, 10)
        self.create_subscription(String, "/custom_task_completion", self.completion_callback, 10)
        
        # æœåŠ¡å®¢æˆ·ç«¯
        self.nav_client = self.create_client(SingleNavTask, "/submit_single_nav_task")
        
        # å†³ç­–å®šæ—¶å™¨ï¼ˆ1ç§’/æ¬¡ï¼‰
        self.create_timer(1.0, self.infer_dispatch_callback)
        
        # å»é‡ç¼“å­˜
        self.processed_ids = set()
        
        self.get_logger().info("âœ… ROS æ¥å£è¿æ¥å®Œæˆ")

    def _init_rl_model(self):
        """åˆå§‹åŒ– PPO æ¨¡å‹ï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼šå¢å¼ºæ¢ç´¢+é™ä½å¥–åŠ±éœ‡è¡ï¼‰"""
        model_path = "./rl_models/dispatching_ppo_final"
        log_dir = "./rl_dispatching_tb_log/"
        
        if self.mode == "train":
            # è½»é‡åŒ–ç½‘ç»œï¼ˆé¿å…è¿‡æ‹Ÿåˆï¼‰
            policy_kwargs = dict(
                activation_fn=th.nn.ReLU,
                net_arch=dict(pi=[128, 128], vf=[128, 128])
            )
            self.get_logger().info("ğŸ“š åˆå§‹åŒ– PPO æ¨¡å‹ï¼ˆè°ƒä¼˜è¶…å‚æ•°ç‰ˆï¼‰...")
            self.model = PPO(
                "MlpPolicy", 
                self.rl_env, 
                verbose=1, 
                device='cpu', 
                tensorboard_log=log_dir,
                policy_kwargs=policy_kwargs,
                learning_rate=3e-5,        # æé«˜å­¦ä¹ ç‡ï¼ˆä»1e-5â†’3e-5ï¼‰
                n_steps=2048,              
                batch_size=256,            
                gamma=0.98,                # æé«˜gammaï¼ˆä»0.95â†’0.98ï¼‰ï¼Œå¢å¼ºé•¿æœŸå¥–åŠ±æƒé‡
                gae_lambda=0.95,           # æé«˜GAEï¼ˆä»0.9â†’0.95ï¼‰
                ent_coef=0.1,              # é™ä½æ¢ç´¢ï¼ˆä»0.2â†’0.1ï¼‰ï¼Œå‡å°‘æ— æ„ä¹‰ç­‰å¾…
                vf_coef=0.5,               
                max_grad_norm=0.5,         
                n_epochs=5,                # æé«˜è¿­ä»£æ¬¡æ•°ï¼ˆä»3â†’5ï¼‰ï¼Œå¢å¼ºå­¦ä¹ æ•ˆæœ
                clip_range=0.2,            # æé«˜è£å‰ªèŒƒå›´ï¼ˆä»0.15â†’0.2ï¼‰
                clip_range_vf=None,        
            )


            # å›è°ƒå‡½æ•°ï¼šç›‘æ§æ”¶æ•›ï¼Œä¿å­˜æœ€ä¼˜æ¨¡å‹
            self.checkpoint_callback = CheckpointCallback(save_freq=20000, save_path="./rl_models/", name_prefix="ppo")
            
            # å¥–åŠ±é˜ˆå€¼åœæ­¢å›è°ƒ
            self.stop_callback = StopTrainingOnRewardThreshold(
                reward_threshold=1000.0,   # é™ä½å¥–åŠ±é˜ˆå€¼ï¼ˆé€‚é…å½’ä¸€åŒ–åçš„å¥–åŠ±ï¼‰
                verbose=1
            )
            
            # æ–°å»ºç‹¬ç«‹çš„éªŒè¯ç¯å¢ƒ
            self.eval_env = Monitor(MockRMFEnv())  # å…³é”®ï¼šç”¨self.eval_envï¼Œå˜æˆç±»å±æ€§
            self.eval_callback = EvalCallback(
                self.eval_env,  # åŒæ­¥ä¿®æ”¹ä¸ºself.eval_env
                eval_freq=5000,
                n_eval_episodes=5,
                best_model_save_path="./rl_models/best/",
                verbose=1,
                callback_after_eval=self.stop_callback,
                deterministic=False,
                render=False
            )

            
            class RewardNormalizationCallback(BaseCallback):
                def __init__(self, verbose=0):
                    super().__init__(verbose)
                    self.reward_sum = 0.0
                    self.reward_count = 0
                    self.reward_mean = 0.0
                    self.reward_std = 1.0

                def _on_step(self) -> bool:
                    # æ”¶é›†å¥–åŠ±ï¼ˆæ¯æ­¥éƒ½æ”¶é›†ï¼Œè€Œéä»…episodeç»“æŸï¼‰
                    if "rewards" in self.locals:
                        current_rewards = self.locals["rewards"]
                        self.reward_sum += np.sum(current_rewards)
                        self.reward_count += len(current_rewards)
                        
                        # æ¯500æ­¥æ›´æ–°ä¸€æ¬¡å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆæ›´é¢‘ç¹ï¼Œé™ä½æ³¢åŠ¨ï¼‰
                        if self.reward_count % 500 == 0:
                            self.reward_mean = self.reward_sum / self.reward_count
                            # é˜²æ­¢æ ‡å‡†å·®ä¸º0
                            self.reward_std = max(1e-6, np.std(self.locals["rewards"]))
                            self.reward_sum = 0.0
                            self.reward_count = 0
                    
                    # å½’ä¸€åŒ–å¥–åŠ±ï¼ˆä½¿ç”¨æ»‘åŠ¨å‡å€¼å’Œæ ‡å‡†å·®ï¼‰
                    if self.reward_std > 0 and "rewards" in self.locals:
                        self.locals["rewards"] = (self.locals["rewards"] - self.reward_mean) / (self.reward_std + 1e-8)
                    return True


            self.reward_norm_callback = RewardNormalizationCallback()
            
        elif self.mode == "infer":
            if os.path.exists(model_path + ".zip"):
                self.get_logger().info(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {model_path}")
                self.model = PPO.load(model_path, device='cpu')
            else:
                self.get_logger().error(f"âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}.zip")
                self.model = None

    # è®­ç»ƒé€»è¾‘ï¼ˆæ–°å¢å¥–åŠ±å½’ä¸€åŒ–å›è°ƒï¼‰
    def start_training(self, total_timesteps=1000000):
        if self.mode != "train": return
            
        self.get_logger().info(f"ğŸ”¥ å¼€å§‹è®­ç»ƒ {total_timesteps} æ­¥ï¼ˆ8ä»»åŠ¡å¿…é¡»å®Œæˆç‰ˆï¼‰...")
        start_t = time.time()  # ä¿ç•™åŸå¼€å§‹æ—¶é—´
        
        self.model.learn(
            total_timesteps=total_timesteps, 
            callback=[self.checkpoint_callback, self.eval_callback, self.reward_norm_callback]
        )
        
        # æ–°å¢ï¼šè®¡ç®—å¹¶æ‰“å°è®­ç»ƒç”¨æ—¶
        end_t = time.time()
        train_duration = end_t - start_t
        hours = int(train_duration // 3600)
        minutes = int((train_duration % 3600) // 60)
        seconds = int(train_duration % 60)
        self.get_logger().info(f"â±ï¸ è®­ç»ƒæ€»ç”¨æ—¶ï¼š{hours}å°æ—¶{minutes}åˆ†é’Ÿ{seconds}ç§’")
        
        # ä¿®æ”¹ï¼šç”¨åŒ…è£…åçš„eval_envè¯„ä¼°ï¼ˆè€Œä¸æ˜¯self.rl_envï¼‰
        mean_reward, std_reward = evaluate_policy(self.model, self.eval_env, n_eval_episodes=10)
        self.get_logger().info(f"ğŸ“Š æœ€ç»ˆæ¨¡å‹è¯„ä¼°ï¼šå¹³å‡å¥–åŠ±={mean_reward:.2f}ï¼Œæ ‡å‡†å·®={std_reward:.2f}")
        
        self.model.save("./rl_models/dispatching_ppo_final")
        self.get_logger().info("ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ï¼")
        rclpy.shutdown()


    # æ¨ç†é€»è¾‘ï¼ˆé€‚é…ä»»åŠ¡æ’é˜Ÿï¼Œç§»é™¤é€‰å¿™ç¢Œè½¦çš„æ ¡éªŒï¼‰
    def infer_dispatch_callback(self):
        # 1. è¿‡æ»¤æœ‰æ•ˆä»»åŠ¡
        valid_tasks = [t for t in self.rl_env.pending_tasks if t["task_id"].startswith("red_cube_")]
        if not valid_tasks:
            self.get_logger().debug("ğŸ“­ æ— æœ‰æ•ˆä»»åŠ¡ï¼Œè·³è¿‡è°ƒåº¦")
            return
        
        # 2. è·å–ç¯å¢ƒçŠ¶æ€ & æ¨¡å‹å†³ç­–
        obs = self.rl_env._get_observation()
        wait_action_idx = len(self.rl_env.robot_names)
        
        if self.model is None:
            self.get_logger().error("âŒ æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•å†³ç­–")
            return
        
        # æ¨¡å‹è¾“å‡ºåŠ¨ä½œï¼ˆå®Œå…¨ä¿¡ä»»æ¨¡å‹çš„å†³ç­–ï¼‰
        action, _states = self.model.predict(obs, deterministic=True)
        self.get_logger().info(f"ğŸ¤– RLæ™ºèƒ½ä½“è¾“å‡ºåŠ¨ä½œï¼š{action} (ç­‰å¾…åŠ¨ä½œç´¢å¼•ï¼š{wait_action_idx})")

        # 3. å¤„ç†æ¨¡å‹è¾“å‡ºçš„åŠ¨ä½œ
        if action == wait_action_idx:
            # åŠ¨ä½œï¼šç­‰å¾…
            self.get_logger().info("â³ RLå†³ç­–ï¼šç­‰å¾…æ–°ä»»åŠ¡")
            for task in self.rl_env.pending_tasks:
                task["wait_time"] += 1.0
        else:
            # åŠ¨ä½œï¼šé€‰è½¦ï¼ˆç›´æ¥æ‰§è¡Œï¼Œæ— è®ºå°è½¦æ˜¯å¦å¿™ç¢Œï¼‰
            selected_robot = self.rl_env.robot_names[action]
            
            # é€‰ç¦»è¯¥å°è½¦æœ€è¿‘çš„ä»»åŠ¡
            min_dist = float("inf")
            best_task = None
            for task in valid_tasks:
                rx = self.rl_env.robot_positions[selected_robot].x
                ry = self.rl_env.robot_positions[selected_robot].y
                dist = np.sqrt((rx - task["x"])**2 + (ry - task["y"])**2)
                if dist < min_dist:
                    min_dist = dist
                    best_task = task
            
            if best_task:
                self._call_ros_service(selected_robot, best_task["task_id"], best_task["waypoint"])

    def _call_ros_service(self, robot, tid, wp):
        """è°ƒç”¨ROSæœåŠ¡ä¸‹å‘ä»»åŠ¡"""
        if not self.nav_client.service_is_ready():
            self.get_logger().warn("âš ï¸ æœåŠ¡æœªå°±ç»ªï¼Œè·³è¿‡ä»»åŠ¡ä¸‹å‘")
            return
            
        req = SingleNavTask.Request()
        req.target_waypoint = wp
        req.fleet_name = "deliveryRobot"
        req.robot_name = robot
        req.priority = 1
        
        # ä¼ é€’wpåˆ°å›è°ƒå‡½æ•°
        future = self.nav_client.call_async(req)
        future.add_done_callback(lambda f, r=robot, t=tid, w=wp: self._service_done(f, t, r, w))

    def _service_done(self, future, tid, robot, wp):
        """æœåŠ¡è°ƒç”¨å®Œæˆå›è°ƒï¼ˆä¿®å¤wpå˜é‡ä¼ é€’+waypoint_coordså½’å±é”™è¯¯ï¼‰"""
        try:
            res = future.result()
            if res.success:
                self.get_logger().info(f"ğŸš€ è°ƒåº¦æˆåŠŸï¼š{robot} -> {tid} @ {res.message}")
                for i, t in enumerate(self.rl_env.pending_tasks):
                    if t["task_id"] == tid:
                        self.rl_env.pending_tasks.pop(i)
                        # æ ‡è®°å°è½¦ä¸ºå¿™ç¢Œï¼ˆå¦‚æœå½“å‰æ— ä»»åŠ¡ï¼‰
                        if self.rl_env.robot_states[robot]["idle"]:
                            self.rl_env.robot_states[robot]["idle"] = False
                        # ä»»åŠ¡åŠ å…¥é˜Ÿåˆ—ï¼ˆä¸ç¯å¢ƒé€»è¾‘å¯¹é½ï¼‰
                        waypoint_x = self.waypoint_coords[wp].x
                        waypoint_y = self.waypoint_coords[wp].y
                        robot_x = self.rl_env.robot_positions[robot].x
                        robot_y = self.rl_env.robot_positions[robot].y
                        
                        self.rl_env.robot_states[robot]["task_queue"].append({
                            "task_id": tid,
                            "waypoint": wp,
                            "x": waypoint_x,
                            "y": waypoint_y,
                            "exec_time": np.sqrt(
                                (robot_x - waypoint_x)**2 + (robot_y - waypoint_y)**2
                            ) / 1.0
                        })
                        break
            else:
                self.get_logger().error(f"âŒ è°ƒåº¦å¤±è´¥ï¼š{tid} -> {res.message}")
        except Exception as e:
            self.get_logger().error(f"âŒ æœåŠ¡è°ƒç”¨å¼‚å¸¸ï¼š{e}")

    # ROS å›è°ƒå‡½æ•°ï¼ˆç®€åŒ–å°è½¦çŠ¶æ€åˆ¤æ–­ï¼‰
    def fleet_state_callback(self, msg):
        if msg.name != "deliveryRobot": return
        for r in msg.robots:
            if r.name in self.rl_env.robot_names:
                # æ›´æ–°ä½ç½®å’ŒåŸºç¡€ç©ºé—²çŠ¶æ€
                self.rl_env.update_robot_position(r.name, r.location.x, r.location.y)
                self.rl_env.robot_states[r.name]["idle"] = not bool(r.task_id)
                if self.rl_env.robot_states[r.name]["idle"]:
                    self.rl_env.robot_states[r.name]["current_task_remaining_time"] = 0.0

    def target_callback(self, msg):
        data = msg.data.split(",")
        if len(data) < 2:
            self.get_logger().warn(f"âš ï¸ æ— æ•ˆä»»åŠ¡æ ¼å¼ï¼š{msg.data}")
            return
        
        tid, wp = data[0].strip(), data[1].strip()
        if not tid.startswith("red_cube_") or tid in self.processed_ids:
            return
        
        if wp in self.waypoint_coords:
            p = self.waypoint_coords[wp]
            self.rl_env.add_task(tid, wp, p.x, p.y)
            self.processed_ids.add(tid)
            self.get_logger().info(f"ğŸ“¥ æ–°å¢ä»»åŠ¡ï¼š{tid} @ {wp}")
        else:
            self.get_logger().error(f"âŒ æœªçŸ¥èˆªç‚¹ï¼š{wp}ï¼Œä»»åŠ¡{tid}æ·»åŠ å¤±è´¥")

    def completion_callback(self, msg):
        data = msg.data.split(",")
        if len(data) < 1:
            self.get_logger().warn(f"âš ï¸ æ— æ•ˆçš„å®Œæˆæ¶ˆæ¯ï¼š{msg.data}")
            return
        tid = data[0].strip()
        
        if tid.startswith("red_cube_"):
            # è°ƒç”¨ç¯å¢ƒçš„å®Œæˆä»»åŠ¡æ–¹æ³•
            self.rl_env.complete_task(tid)
            # æ¸…ç†ç¼“å­˜
            if tid in self.processed_ids:
                self.processed_ids.remove(tid)
            # æ ‡è®°å°è½¦ç©ºé—²ï¼ˆå…œåº•ï¼‰
            for robot in self.rl_env.robot_names:
                if not self.rl_env.robot_states[robot]["idle"] and self.rl_env.robot_states[robot]["current_target"]:
                    if self.rl_env.robot_states[robot]["current_target"]["task_id"] == tid:
                        self.rl_env.robot_states[robot]["idle"] = True
                        self.rl_env.robot_states[robot]["current_target"] = None
                        self.rl_env.robot_states[robot]["current_task_remaining_time"] = 0.0
                        break
                # æ£€æŸ¥æ’é˜Ÿä»»åŠ¡
                for i, task in enumerate(self.rl_env.robot_states[robot]["task_queue"]):
                    if task["task_id"] == tid:
                        self.rl_env.robot_states[robot]["task_queue"].pop(i)
                        break

# ä¸»ç¨‹åº
def main(args=None):
    rclpy.init(args=args)
    node = RLDispatcherNode()
    
    if node.mode == "train":
        train_thread = threading.Thread(
            target=node.start_training, 
            args=(1000000,),
            daemon=True
        )
        train_thread.start()
        try:
            rclpy.spin(node)
        except SystemExit:
            pass
        train_thread.join()
    else:
        try:
            rclpy.spin(node)
        except KeyboardInterrupt:
            pass
        
    node.destroy_node()
    if rclpy.ok():
        rclpy.shutdown()

if __name__ == "__main__":
    main()
