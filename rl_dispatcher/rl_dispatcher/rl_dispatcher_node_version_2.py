#!/usr/bin/env python3

# 1.22ã€æ¶ˆèå®éªŒã€‘ç§»é™¤é”šç‚¹é€»è¾‘ï¼Œç»Ÿä¸€ä½¿ç”¨å°è½¦å®æ—¶ä½ç½® 10min10s
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import rclpy
from rclpy.node import Node
from rmf_fleet_msgs.msg import FleetState
from std_msgs.msg import String
from geometry_msgs.msg import Point
from rmf_custom_tasks_self.srv import SingleNavTask
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, StopTrainingOnRewardThreshold, BaseCallback
from stable_baselines3.common.evaluation import evaluate_policy
import time
import random
import threading
import torch as th
from stable_baselines3.common.monitor import Monitor

import gymnasium as gym
from gymnasium import spaces

# å¼•å…¥æ¨¡æ‹Ÿç¯å¢ƒ
try:
    from mock_env import MockRMFEnv
except ImportError:
    print("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° mock_env.pyï¼Œè¯·ç¡®ä¿æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼")
    sys.exit(1)

# ===================== çœŸå® ROS ç¯å¢ƒ =====================
class RLDispatchingEnv(gym.Env):
    """
    çœŸå®RMFç¯å¢ƒï¼šå®Œå…¨å¯¹é½Mocké€»è¾‘  
    ã€æ¶ˆèå®éªŒã€‘ç§»é™¤é”šç‚¹é€»è¾‘ï¼Œç»Ÿä¸€ä½¿ç”¨å°è½¦å®æ—¶ä½ç½®
    """
    metadata = {"render_modes": ["human"], "render_fps": 1}
    
    def __init__(self, node, render_mode=None):
        super().__init__()
        self.node = node
        self.render_mode = render_mode
        
        self.robot_names = ["deliveryRobot_0", "deliveryRobot_1", "deliveryRobot_2"]
        self.robot_positions = {
            "deliveryRobot_0": Point(x=96.59527587890625, y=-51.96450424194336),
            "deliveryRobot_1": Point(x=152.3477325439453, y=-44.31863021850586),
            "deliveryRobot_2": Point(x=14.776845932006836, y=-9.279278755187988)
        }
        self.robot_states = {
            name: {
                "idle": True,
                "current_target": None,
                "task_queue": [],
                "current_task_remaining_time": 0.0,
                "speed": 1.0 # å‡è®¾é€Ÿåº¦ï¼Œç”¨äºä¼°ç®—æ—¶é—´
            } for name in self.robot_names
        }
        
        self.pending_tasks = []
        self.completed_tasks = []
        self.task_timeout_count = {}
        
        self.current_time = 0.0
        self.episode_steps = 0
        self.max_episode_steps = 1000
        self.total_reward = 0.0
        
        # å‡çº§ï¼š54ç»´çŠ¶æ€ç©ºé—´
        self.action_space = spaces.Discrete(len(self.robot_names) + 1)
        self.observation_space = spaces.Box(low=-1.0, high=1.0, shape=(54,), dtype=np.float32)
        
        # 7. å¥–åŠ±ç³»æ•° (âš¡ï¸ ä¼‘å…‹ç–—æ³•ç‰ˆ - ç”¨äºæ¨ç†é˜¶æ®µæ—¥å¿—æ˜¾ç¤º âš¡ï¸)
        self.reward_coeff = {
            "valid_assign": 5.0,
            "wait_with_task": -2.0,
            "wait_no_task": 0.1,
            "distance": -0.005,
            "time_congestion": -0.01,
            "queue_congestion": -0.2,
            "step": -0.01,
            "invalid_selection": -5.0,
            "completion": 5.0,
            "all_completed": 20.0,
            "batch_completion": 2.0
        }

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        for name in self.robot_names:
            self.robot_states[name] = {
                "idle": True,
                "current_target": None,
                "task_queue": [],
                "current_task_remaining_time": 0.0,
                "speed": 1.0
            }
        self.pending_tasks = []
        self.completed_tasks = []
        self.task_timeout_count = {}
        self.current_time = 0.0
        self.episode_steps = 0
        self.total_reward = 0.0
        return self._get_observation(), {"total_reward": 0.0}

    def _get_observation(self):
        """54ç»´çŠ¶æ€å‘é‡ï¼šå¯¹é½MockEnvçš„æ–°ç‰¹å¾
        ã€æ¶ˆèä¿®æ”¹1ã€‘ç§»é™¤æœ«ç«¯é”šç‚¹è®¡ç®—ï¼Œç»Ÿä¸€ç”¨å°è½¦å®æ—¶ä½ç½®
        """
        robot_obs = []
        for name in self.robot_names:
            r = self.robot_states[name]
            pos = self.robot_positions[name]
            
            # ========== ã€æ¶ˆèä¿®æ”¹1ã€‘ç§»é™¤é”šç‚¹é€»è¾‘ï¼Œç›´æ¥ä½¿ç”¨å°è½¦å®æ—¶ä½ç½® ==========
            last_x, last_y = pos.x, pos.y
            
            # 2. è®¡ç®—é¢„è®¡æ€»è´Ÿè·æ—¶é—´
            total_time = r["current_task_remaining_time"]
            for t in r["task_queue"]:
                total_time += t["exec_time"]

            queue_len = len(r["task_queue"]) if not r["idle"] else 0.0
            robot_obs.extend([
                pos.x / 200.0,
                pos.y / 100.0,
                1.0 if r["idle"] else 0.0,
                min(queue_len / 5.0, 1.0),
                last_x / 200.0,                 # New
                last_y / 100.0,                 # New
                min(total_time / 200.0, 1.0)    # New
            ])
        
        task_obs = []
        for _ in range(8):
            task_obs.extend([0.0, 0.0, 0.0])
        for i, task in enumerate(self.pending_tasks[:8]):
            task_obs[i*3] = task["x"] / 200.0
            task_obs[i*3+1] = task["y"] / 100.0
            task_obs[i*3+2] = min(task["wait_time"] / 600.0, 1.0)
        
        total_queue_len = sum([len(self.robot_states[name]["task_queue"]) for name in self.robot_names])
        idle_robot_count = sum([1 for name in self.robot_names if self.robot_states[name]["idle"]])
        queue_lengths = [len(self.robot_states[name]["task_queue"]) for name in self.robot_names]
        max_queue_len = max(queue_lengths) if queue_lengths else 0
        min_queue_len = min(queue_lengths) if queue_lengths else 0
        
        global_obs = [
            self.current_time / 1000.0,
            len(self.completed_tasks) / 8.0,
            len(self.pending_tasks) / 8.0,
            idle_robot_count / 3.0,
            total_queue_len / 15.0,
            (len(self.completed_tasks) + len(self.pending_tasks)) / 8.0,
            np.var(queue_lengths) / 2.5 if queue_lengths else 0.0,
            max_queue_len / 5.0,
            (max_queue_len - min_queue_len) / 5.0 if queue_lengths else 0.0,
        ]
        global_obs = [min(max(x, -1.0), 1.0) for x in global_obs]
        
        return np.array(robot_obs + task_obs + global_obs, dtype=np.float32)

    def step(self, action):
        self.episode_steps += 1
        self.current_time += 1.0
        # åœ¨æ¨ç†æ¨¡å¼ä¸‹ï¼ŒStepçš„å¥–åŠ±è®¡ç®—ä»…ç”¨äºæ—¥å¿—è®°å½•
        reward = self.reward_coeff["step"]

        def clip_reward(r, min_r=-20.0, max_r=20.0):
            return max(min(r, max_r), min_r)
        
        wait_action_idx = len(self.robot_names)
        info = {
            "action_type": "wait" if action == wait_action_idx else "assign",
            "selected_action": action,
            "robot_task_queues": {name: len(self.robot_states[name]["task_queue"]) for name in self.robot_names}
        }
        
        if action == wait_action_idx:
            if self.pending_tasks:
                reward += self.reward_coeff["wait_with_task"]
                for task in self.pending_tasks:
                    task["wait_time"] += 1.0
            else:
                reward += self.reward_coeff["wait_no_task"]
        else:
            selected_robot = self.robot_names[action]
            info["selected_robot"] = selected_robot
            robot = self.robot_states[selected_robot]
            
            if not self.pending_tasks:
                reward += self.reward_coeff["invalid_selection"]
                return self._get_observation(), reward, False, self.episode_steps >= self.max_episode_steps, info
            
            # æœ‰æ•ˆæ´¾å•
            reward += self.reward_coeff["valid_assign"]

            # ========== ã€æ¶ˆèä¿®æ”¹2ã€‘ç§»é™¤é”šç‚¹é€»è¾‘ï¼Œç›´æ¥ä½¿ç”¨å°è½¦å®æ—¶ä½ç½® ==========
            anchor_x = self.robot_positions[selected_robot].x
            anchor_y = self.robot_positions[selected_robot].y

            # å¯»æ‰¾æœ€è¿‘ä»»åŠ¡
            min_dist = float("inf")
            best_task_idx = 0
            for i, task in enumerate(self.pending_tasks):
                dist = np.sqrt((anchor_x - task["x"])**2 + (anchor_y - task["y"])**2)
                if dist < min_dist:
                    min_dist = dist
                    best_task_idx = i
            
            task = self.pending_tasks.pop(best_task_idx)
            task_id = task["task_id"]
            info["task_id"] = task_id
            
            # --- å¥–åŠ±è®¡ç®— ---
            reward += self.reward_coeff["distance"] * min_dist
            
            wait_time_for_new_task = robot["current_task_remaining_time"] + \
                                     sum([t["exec_time"] for t in robot["task_queue"]])
            reward += self.reward_coeff["time_congestion"] * wait_time_for_new_task
            
            new_queue_len = len(robot["task_queue"]) + (1 if not robot["idle"] else 0)
            reward += self.reward_coeff["queue_congestion"] * (new_queue_len ** 2)
            
            task_exec_time = min_dist / 1.0
            task_info = {
                "task_id": task_id,
                "waypoint": task["waypoint"],
                "x": task["x"],
                "y": task["y"],
                "exec_time": task_exec_time
            }
            
            if robot["idle"]:
                robot["idle"] = False
                robot["current_target"] = task_info
                robot["current_task_remaining_time"] = task_exec_time
            else:
                robot["task_queue"].append(task_info)
                # ========== ã€æ¶ˆèä¿®æ”¹3ã€‘é˜Ÿåˆ—é‡æ’åº - ç§»é™¤é”šç‚¹ï¼Œç”¨å°è½¦å®æ—¶ä½ç½® ==========
                anchor_x, anchor_y = self.robot_positions[selected_robot].x, self.robot_positions[selected_robot].y
                
                def sort_by_distance(task):
                    return np.sqrt((anchor_x - task["x"])**2 + (anchor_y - task["y"])**2)
                
                robot["task_queue"].sort(key=sort_by_distance)
        
        # ROSæ¨¡æ‹Ÿæ‰§è¡Œæ›´æ–° (éƒ¨åˆ†é€»è¾‘åœ¨å›è°ƒä¸­å¤„ç†)
        task_completion_rewards = 0
        for name in self.robot_names:
            r = self.robot_states[name]
            if not r["idle"] and r["current_target"]:
                r["current_task_remaining_time"] -= 1.0
                r["current_task_remaining_time"] = max(0.0, r["current_task_remaining_time"])
                # ROSç¯å¢ƒä¸­ä»»åŠ¡å®Œæˆä¸»è¦ç”±å›è°ƒè§¦å‘

        reward += task_completion_rewards
        reward = clip_reward(reward)
        self.total_reward += reward
        truncated = self.episode_steps >= self.max_episode_steps
        terminated = (len(self.pending_tasks) == 0 and 
                      len(self.completed_tasks) == 8 and
                      all([r["idle"] and len(r["task_queue"]) == 0 for r in self.robot_states.values()]))
        
        return self._get_observation(), reward, terminated, truncated, info

    def update_robot_position(self, name, x, y):
        if name in self.robot_positions:
            self.robot_positions[name].x = x
            self.robot_positions[name].y = y
            
    def add_task(self, tid, wp, x, y):
        self.pending_tasks.append({
            "task_id": tid, 
            "waypoint": wp, 
            "x": x, 
            "y": y, 
            "wait_time": 0.0
        })
        self.task_timeout_count[tid] = 0
        
    def complete_task(self, tid):
        target_robot = None
        for name in self.robot_names:
            r = self.robot_states[name]
            if not r["idle"] and r["current_target"] and r["current_target"]["task_id"] == tid:
                target_robot = name
                break
            for i, task in enumerate(r["task_queue"]):
                if task["task_id"] == tid:
                    r["task_queue"].pop(i)
                    return # æ’é˜Ÿä»»åŠ¡å–æ¶ˆ
        
        if target_robot:
            r = self.robot_states[target_robot]
            self.total_reward += self.reward_coeff["completion"]
            self.completed_tasks.append(tid)
            self.node.get_logger().info(f"ğŸ’° ä»»åŠ¡å®Œæˆå¥–åŠ± +{self.reward_coeff['completion']}! ({target_robot})")
            
            if r["task_queue"]:
                next_task = r["task_queue"].pop(0)
                r["current_target"] = next_task
                r["current_task_remaining_time"] = next_task["exec_time"]
            else:
                r["idle"] = True
                r["current_target"] = None
                r["current_task_remaining_time"] = 0.0
            
            if len(self.completed_tasks) == 8:
                self.total_reward += self.reward_coeff["all_completed"]

# ===================== æ ¸å¿ƒè°ƒåº¦èŠ‚ç‚¹ =====================
class RLDispatcherNode(Node):
    def __init__(self):
        super().__init__("rl_dispatcher_node")
        self.get_logger().info("ğŸš€ åˆå§‹åŒ– RL è°ƒåº¦èŠ‚ç‚¹ï¼ˆä¼˜åŒ–ç‰ˆï¼šé“¾å¼æ´¾å•+æ‹¥å µæ„ŸçŸ¥ï¼‰...")
        self.get_logger().warning("âš ï¸ ã€æ¶ˆèå®éªŒã€‘å·²ç§»é™¤é”šç‚¹é€»è¾‘ï¼Œç»Ÿä¸€ä½¿ç”¨å°è½¦å®æ—¶ä½ç½®ï¼")
        
        if not self.has_parameter("use_sim_time"):
            self.declare_parameter("use_sim_time", True)
        self.set_parameters([rclpy.parameter.Parameter("use_sim_time", rclpy.Parameter.Type.BOOL, True)])

        if not self.has_parameter("mode"):
            self.declare_parameter("mode", "train")
        self.mode = self.get_parameter("mode").get_parameter_value().string_value
        self.get_logger().info(f"ğŸ”µ å½“å‰æ¨¡å¼: [{self.mode.upper()}]")
        
        if self.mode == "train":
            self.get_logger().info("ğŸš€ è®­ç»ƒæ¨¡å¼ - æŒ‚è½½ Mock è™šæ‹Ÿå¼•æ“")
            self.rl_env = MockRMFEnv() 
        elif self.mode == "infer":
            self.get_logger().info("ğŸ¤– æ¨ç†æ¨¡å¼ - æŒ‚è½½ Real çœŸå®å¼•æ“")
            self.rl_env = RLDispatchingEnv(self, render_mode="human")
        else:
            self.get_logger().error("âŒ æœªçŸ¥æ¨¡å¼ï¼è¯·ä½¿ç”¨ mode:=train æˆ– mode:=infer")
            sys.exit(1)

        self.model = None
        self._init_rl_model()
        
        if self.mode == "infer":
            self._init_ros_interfaces()

    def _init_ros_interfaces(self):
        self.get_logger().info("ğŸ”Œ è¿æ¥ ROS æ¥å£...")
        
        self.waypoint_coords = {
            "n14": Point(x=80.84, y=-28.52), "n13": Point(x=84.44, y=-4.94),
            "n23": Point(x=182.80, y=-42.30), "s08": Point(x=96.61, y=-50.50),
            "s10": Point(x=122.10, y=-46.68), "west_koi_pond": Point(x=34.32, y=-10.13),
            "n08": Point(x=59.61, y=-7.42), "junction_south_west": Point(x=84.56, y=-38.81)
        }

        self.create_subscription(FleetState, "/fleet_states", self.fleet_state_callback, 10)
        self.create_subscription(String, "/task_monitor/start", self.target_callback, 10)
        self.create_subscription(String, "/custom_task_completion", self.completion_callback, 10)
        self.nav_client = self.create_client(SingleNavTask, "/submit_single_nav_task")
        self.create_timer(1.0, self.infer_dispatch_callback)
        self.processed_ids = set()
        
        self.get_logger().info("âœ… ROS æ¥å£è¿æ¥å®Œæˆ")

    def _init_rl_model(self):
        model_path = "./rl_models/dispatching_ppo_final_no_anchor"  # ã€æ¶ˆèä¿®æ”¹ã€‘ä¿®æ”¹æ¨¡å‹ä¿å­˜è·¯å¾„
        log_dir = "./rl_dispatching_tb_log_no_anchor/"              # ã€æ¶ˆèä¿®æ”¹ã€‘ä¿®æ”¹æ—¥å¿—è·¯å¾„
        
        if self.mode == "train":
            policy_kwargs = dict(
                activation_fn=th.nn.ReLU,
                net_arch=dict(pi=[128, 128], vf=[128, 128])
            )
            self.get_logger().info("ğŸ“š åˆå§‹åŒ– PPO æ¨¡å‹...")
            
            # â¬‡ï¸â¬‡ï¸â¬‡ï¸ å…³é”®ä¿®æ”¹ï¼šé™ä½ç†µç³»æ•° (ent_coef)ï¼Œç§»é™¤å½’ä¸€åŒ–
            self.model = PPO(
                "MlpPolicy", 
                self.rl_env, 
                verbose=1, 
                device='cpu', 
                tensorboard_log=log_dir,
                policy_kwargs=policy_kwargs,
                learning_rate=3e-4, # æé«˜å­¦ä¹ ç‡
                n_steps=2048,
                batch_size=64, # å‡å°batchè®©æ›´æ–°æ›´é¢‘ç¹
                gamma=0.95, # é™ä½è¿œæœŸæŠ˜æ‰£
                gae_lambda=0.95,
                ent_coef=0.01, # é™ä½éšæœºæ€§ (ä¹‹å‰æ˜¯0.1)
                vf_coef=0.5,
                max_grad_norm=0.5,
                n_epochs=10, # å¢åŠ æ›´æ–°è½®æ•°
                clip_range=0.2
            )

            self.checkpoint_callback = CheckpointCallback(save_freq=10000, save_path="./rl_models/no_anchor/", name_prefix="ppo")  # ã€æ¶ˆèä¿®æ”¹ã€‘
            self.stop_callback = StopTrainingOnRewardThreshold(reward_threshold=1000.0, verbose=1)
            
            self.eval_env = Monitor(MockRMFEnv())
            self.eval_callback = EvalCallback(
                self.eval_env,
                eval_freq=5000,
                n_eval_episodes=5,
                best_model_save_path="./rl_models/best/no_anchor/",  # ã€æ¶ˆèä¿®æ”¹ã€‘
                verbose=1,
                callback_after_eval=self.stop_callback,
                deterministic=False,
                render=False
            )
            # âš¡ï¸ å·²ç§»é™¤ RewardNormalizationCallback

        elif self.mode == "infer":
            if os.path.exists(model_path + ".zip"):
                self.get_logger().info(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {model_path}")
                self.model = PPO.load(model_path, device='cpu')
            else:
                self.get_logger().error(f"âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}.zip")
                self.model = None

    def start_training(self, total_timesteps=1000000):
        if self.mode != "train": return
            
        self.get_logger().info(f"ğŸ”¥ å¼€å§‹ä¼‘å…‹ç–—æ³•è®­ç»ƒ {total_timesteps} æ­¥...")
        self.get_logger().warning("âš ï¸ ã€æ¶ˆèå®éªŒã€‘è®­ç»ƒæ— é”šç‚¹é€»è¾‘çš„æ¨¡å‹ï¼")
        start_t = time.time()
        
        self.model.learn(
            total_timesteps=total_timesteps, 
            callback=[self.checkpoint_callback, self.eval_callback] # ç§»é™¤å½’ä¸€åŒ–å›è°ƒ
        )
        
        end_t = time.time()
        train_duration = end_t - start_t
        hours = int(train_duration // 3600)
        minutes = int((train_duration % 3600) // 60)
        seconds = int(train_duration % 60)
        self.get_logger().info(f"â±ï¸ è®­ç»ƒæ€»ç”¨æ—¶ï¼š{hours}å°æ—¶{minutes}åˆ†é’Ÿ{seconds}ç§’")
        
        mean_reward, std_reward = evaluate_policy(self.model, self.eval_env, n_eval_episodes=10)
        self.get_logger().info(f"ğŸ“Š æœ€ç»ˆæ¨¡å‹è¯„ä¼°ï¼šå¹³å‡å¥–åŠ±={mean_reward:.2f}ï¼Œæ ‡å‡†å·®={std_reward:.2f}")
        
        self.model.save("./rl_models/dispatching_ppo_final_no_anchor")  # ã€æ¶ˆèä¿®æ”¹ã€‘
        self.get_logger().info("ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ï¼")
        rclpy.shutdown()

    def infer_dispatch_callback(self):
        valid_tasks = [t for t in self.rl_env.pending_tasks if t["task_id"].startswith("red_cube_")]
        if not valid_tasks:
            self.get_logger().debug("ğŸ“­ æ— æœ‰æ•ˆä»»åŠ¡ï¼Œè·³è¿‡è°ƒåº¦")
            return
        
        obs = self.rl_env._get_observation()
        wait_action_idx = len(self.rl_env.robot_names)
        
        if self.model is None:
            self.get_logger().error("âŒ æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•å†³ç­–")
            return
        
        action, _states = self.model.predict(obs, deterministic=True)
        self.get_logger().info(f"ğŸ¤– RLæ™ºèƒ½ä½“è¾“å‡ºåŠ¨ä½œï¼š{action} (ç­‰å¾…åŠ¨ä½œç´¢å¼•ï¼š{wait_action_idx})")

        if action == wait_action_idx:
            # å¢åŠ å¯¹å·æ‡’è¡Œä¸ºçš„æ£€æµ‹æ—¥å¿—
            if valid_tasks:
                 self.get_logger().warn(f"ğŸ˜¡ æ¨¡å‹æƒ³å·æ‡’(Wait)ï¼Œä½†å½“å‰æœ‰ {len(valid_tasks)} ä¸ªä»»åŠ¡ï¼")
            else:
                 self.get_logger().info("â³ RLå†³ç­–ï¼šç­‰å¾…æ–°ä»»åŠ¡")
            
            for task in self.rl_env.pending_tasks:
                task["wait_time"] += 1.0
        else:
            selected_robot = self.rl_env.robot_names[action]
            robot = self.rl_env.robot_states[selected_robot]
            
            # ========== ã€æ¶ˆèä¿®æ”¹4ã€‘ç§»é™¤é”šç‚¹é€»è¾‘ï¼Œç›´æ¥ä½¿ç”¨å°è½¦å®æ—¶ä½ç½® ==========
            anchor_x = self.rl_env.robot_positions[selected_robot].x
            anchor_y = self.rl_env.robot_positions[selected_robot].y

            min_dist = float("inf")
            best_task = None
            for task in valid_tasks:
                dist = np.sqrt((anchor_x - task["x"])**2 + (anchor_y - task["y"])**2)
                if dist < min_dist:
                    min_dist = dist
                    best_task = task
            
            if best_task:
                self._call_ros_service(selected_robot, best_task["task_id"], best_task["waypoint"])

    def _call_ros_service(self, robot, tid, wp):
        if not self.nav_client.service_is_ready():
            self.get_logger().warn("âš ï¸ æœåŠ¡æœªå°±ç»ªï¼Œè·³è¿‡ä»»åŠ¡ä¸‹å‘")
            return
            
        req = SingleNavTask.Request()
        req.target_waypoint = wp
        req.fleet_name = "deliveryRobot"
        req.robot_name = robot
        req.priority = 1
        
        future = self.nav_client.call_async(req)
        future.add_done_callback(lambda f, r=robot, t=tid, w=wp: self._service_done(f, t, r, w))

    def _service_done(self, future, tid, robot, wp):
        try:
            res = future.result()
            if res.success:
                self.get_logger().info(f"ğŸš€ è°ƒåº¦æˆåŠŸï¼š{robot} -> {tid} @ {res.message}")
                for i, t in enumerate(self.rl_env.pending_tasks):
                    if t["task_id"] == tid:
                        self.rl_env.pending_tasks.pop(i)
                        
                        if self.rl_env.robot_states[robot]["idle"]:
                            self.rl_env.robot_states[robot]["idle"] = False
                        
                        waypoint_x = self.waypoint_coords[wp].x
                        waypoint_y = self.waypoint_coords[wp].y
                        
                        r_state = self.rl_env.robot_states[robot]
                        if r_state["task_queue"]:
                            prev = r_state["task_queue"][-1]
                            dist = np.sqrt((prev["x"] - waypoint_x)**2 + (prev["y"] - waypoint_y)**2)
                        elif r_state["current_target"]:
                            prev = r_state["current_target"]
                            dist = np.sqrt((prev["x"] - waypoint_x)**2 + (prev["y"] - waypoint_y)**2)
                        else:
                            r_pos = self.rl_env.robot_positions[robot]
                            dist = np.sqrt((r_pos.x - waypoint_x)**2 + (r_pos.y - waypoint_y)**2)

                        new_task = {
                            "task_id": tid,
                            "waypoint": wp,
                            "x": waypoint_x,
                            "y": waypoint_y,
                            "exec_time": dist / 1.0
                        }
                        r_state["task_queue"].append(new_task)
                        
                        # ========== ã€æ¶ˆèä¿®æ”¹5ã€‘é˜Ÿåˆ—é‡æ’åº - ç§»é™¤é”šç‚¹ï¼Œç”¨å°è½¦å®æ—¶ä½ç½® ==========
                        anchor_x, anchor_y = self.rl_env.robot_positions[robot].x, self.rl_env.robot_positions[robot].y
                        
                        def sort_by_distance(task):
                            return np.sqrt((anchor_x - task["x"])**2 + (anchor_y - task["y"])**2)
                        
                        r_state["task_queue"].sort(key=sort_by_distance)
                        # =========================================
                        break
            else:
                self.get_logger().error(f"âŒ è°ƒåº¦å¤±è´¥ï¼š{tid} -> {res.message}")
        except Exception as e:
            self.get_logger().error(f"âŒ æœåŠ¡è°ƒç”¨å¼‚å¸¸ï¼š{e}")


    def fleet_state_callback(self, msg):
        if msg.name != "deliveryRobot": return
        for r in msg.robots:
            if r.name in self.rl_env.robot_names:
                self.rl_env.update_robot_position(r.name, r.location.x, r.location.y)
                if not r.task_id:
                     if not self.rl_env.robot_states[r.name]["task_queue"]:
                         self.rl_env.robot_states[r.name]["idle"] = True
                         self.rl_env.robot_states[r.name]["current_task_remaining_time"] = 0.0
                else:
                     self.rl_env.robot_states[r.name]["idle"] = False

    def target_callback(self, msg):
        data = msg.data.split(",")
        if len(data) < 2: return
        tid, wp = data[0].strip(), data[1].strip()
        if not tid.startswith("red_cube_") or tid in self.processed_ids: return
        if wp in self.waypoint_coords:
            p = self.waypoint_coords[wp]
            self.rl_env.add_task(tid, wp, p.x, p.y)
            self.processed_ids.add(tid)
            self.get_logger().info(f"ğŸ“¥ æ–°å¢ä»»åŠ¡ï¼š{tid} @ {wp}")

    def completion_callback(self, msg):
        data = msg.data.split(",")
        if len(data) < 1: return
        tid = data[0].strip()
        if tid.startswith("red_cube_"):
            self.rl_env.complete_task(tid)
            if tid in self.processed_ids:
                self.processed_ids.remove(tid)
            # çŠ¶æ€æ¸…ç†å…œåº•
            for robot in self.rl_env.robot_names:
                r = self.rl_env.robot_states[robot]
                if not r["idle"] and r["current_target"] and r["current_target"]["task_id"] == tid:
                    r["idle"] = True
                    r["current_target"] = None
                    r["current_task_remaining_time"] = 0.0

def main(args=None):
    rclpy.init(args=args)
    node = RLDispatcherNode()
    if node.mode == "train":
        train_thread = threading.Thread(target=node.start_training, args=(1000000,), daemon=True)
        train_thread.start()
        try:
            rclpy.spin(node)
        except SystemExit:
            pass
        train_thread.join()
    else:
        try:
            rclpy.spin(node)
        except KeyboardInterrupt:
            pass
    node.destroy_node()
    if rclpy.ok():
        rclpy.shutdown()

if __name__ == "__main__":
    main()
